"""
Anthropicæ ¼å¼è½¬æ¢å™¨
å¤„ç†Anthropic APIæ ¼å¼ä¸å…¶ä»–æ ¼å¼ä¹‹é—´çš„è½¬æ¢
"""
from typing import Dict, Any, Optional, List
import json
import copy

from .base_converter import BaseConverter, ConversionResult, ConversionError

# å…¨å±€å·¥å…·çŠ¶æ€ç®¡ç†å™¨
class ToolStateManager:
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._tool_mappings = {}
        return cls._instance
    
    def store_tool_mapping(self, func_name: str, tool_id: str):
        """å­˜å‚¨å·¥å…·ååˆ°IDçš„æ˜ å°„"""
        self._tool_mappings[func_name] = tool_id
    
    def get_tool_id(self, func_name: str) -> Optional[str]:
        """æ ¹æ®å·¥å…·åè·å–ID"""
        return self._tool_mappings.get(func_name)
    
    def clear_mappings(self):
        """æ¸…é™¤æ‰€æœ‰æ˜ å°„"""
        self._tool_mappings.clear()

# å…¨å±€å·¥å…·çŠ¶æ€ç®¡ç†å™¨å®ä¾‹
tool_state_manager = ToolStateManager()


class AnthropicConverter(BaseConverter):
    """Anthropicæ ¼å¼è½¬æ¢å™¨"""
    
    def __init__(self):
        super().__init__()
        self.original_model = None
        self._tool_id_mapping = {}  # å­˜å‚¨tool_use_idåˆ°function_nameçš„æ˜ å°„
        
        # ä½¿ç”¨ç»Ÿä¸€çš„æ—¥å¿—è®¾ç½®ï¼ˆç»§æ‰¿è‡ªBaseConverterï¼‰
        # self.logger å·²ç»åœ¨ BaseConverter.__init__() ä¸­æ­£ç¡®è®¾ç½®
    
    def set_original_model(self, model: str):
        """è®¾ç½®åŸå§‹æ¨¡å‹åç§°"""
        self.original_model = model
    
    def _determine_reasoning_effort_from_budget(self, budget_tokens: Optional[int]) -> str:
        """æ ¹æ®budget_tokensæ™ºèƒ½åˆ¤æ–­OpenAI reasoning_effortç­‰çº§
        
        Args:
            budget_tokens: Anthropic thinkingçš„budget_tokenså€¼
            
        Returns:
            str: OpenAI reasoning_effortç­‰çº§ ("low", "medium", "high")
        """
        import os
        
        # å¦‚æœæ²¡æœ‰æä¾›budget_tokensï¼Œé»˜è®¤ä¸ºhigh
        if budget_tokens is None:
            self.logger.info("No budget_tokens provided, defaulting to reasoning_effort='high'")
            return "high"
        
        # ä»ç¯å¢ƒå˜é‡è·å–é˜ˆå€¼é…ç½®
        low_threshold_str = os.environ.get("ANTHROPIC_TO_OPENAI_LOW_REASONING_THRESHOLD")
        high_threshold_str = os.environ.get("ANTHROPIC_TO_OPENAI_HIGH_REASONING_THRESHOLD")
        
        # æ£€æŸ¥å¿…éœ€çš„ç¯å¢ƒå˜é‡
        if low_threshold_str is None:
            raise ConversionError("ANTHROPIC_TO_OPENAI_LOW_REASONING_THRESHOLD environment variable is required for intelligent reasoning_effort determination")
        
        if high_threshold_str is None:
            raise ConversionError("ANTHROPIC_TO_OPENAI_HIGH_REASONING_THRESHOLD environment variable is required for intelligent reasoning_effort determination")
        
        try:
            low_threshold = int(low_threshold_str)
            high_threshold = int(high_threshold_str)
            
            self.logger.debug(f"Threshold configuration: low <= {low_threshold}, medium <= {high_threshold}, high > {high_threshold}")
            
            if budget_tokens <= low_threshold:
                effort = "low"
            elif budget_tokens <= high_threshold:
                effort = "medium"
            else:
                effort = "high"
            
            self.logger.info(f"ğŸ¯ Budget tokens {budget_tokens} -> reasoning_effort '{effort}' (thresholds: low<={low_threshold}, high<={high_threshold})")
            return effort
            
        except ValueError as e:
            raise ConversionError(f"Invalid threshold values in environment variables: {e}. ANTHROPIC_TO_OPENAI_LOW_REASONING_THRESHOLD and ANTHROPIC_TO_OPENAI_HIGH_REASONING_THRESHOLD must be integers.")
    
    def reset_streaming_state(self):
        """é‡ç½®æ‰€æœ‰æµå¼ç›¸å…³çš„çŠ¶æ€å˜é‡ï¼Œé¿å…çŠ¶æ€æ±¡æŸ“"""
        self.logger.debug("reset_streaming_state() called - cleaning up streaming state")
        streaming_attrs = [
            '_streaming_state', '_gemini_sent_start', '_gemini_stream_id', 
            '_gemini_text_started'
        ]
        cleaned_attrs = []
        for attr in streaming_attrs:
            if hasattr(self, attr):
                cleaned_attrs.append(attr)
                delattr(self, attr)
        
        self.logger.debug(f"Cleaned streaming attributes: {cleaned_attrs}")
        
        # å¼ºåˆ¶é‡ç½®ï¼Œç¡®ä¿ä¸‹æ¬¡è®¿é—®æ—¶é‡æ–°åˆå§‹åŒ–
        self._force_reset = True
    
    def get_supported_formats(self) -> List[str]:
        """è·å–æ”¯æŒçš„æ ¼å¼åˆ—è¡¨"""
        return ["openai", "anthropic", "gemini"]
    
    def convert_request(
        self,
        data: Dict[str, Any],
        target_format: str,
        headers: Optional[Dict[str, str]] = None
    ) -> ConversionResult:
        """è½¬æ¢Anthropicè¯·æ±‚åˆ°ç›®æ ‡æ ¼å¼"""
        try:
            if target_format == "anthropic":
                # Anthropicåˆ°Anthropicï¼Œæ ¼å¼ä¸æ¸ é“ç›¸åŒï¼Œä¸éœ€è¦è½¬æ¢æ€è€ƒå‚æ•°
                return ConversionResult(success=True, data=data)
            elif target_format == "openai":
                return self._convert_to_openai_request(data)
            elif target_format == "gemini":
                return self._convert_to_gemini_request(data)
            else:
                return ConversionResult(
                    success=False,
                    error=f"Unsupported target format: {target_format}"
                )
        except Exception as e:
            self.logger.error(f"Failed to convert Anthropic request to {target_format}: {e}")
            return ConversionResult(success=False, error=str(e))
    
    def convert_response(
        self,
        data: Dict[str, Any],
        source_format: str,
        target_format: str
    ) -> ConversionResult:
        """è½¬æ¢å“åº”åˆ°Anthropicæ ¼å¼"""
        try:
            if source_format == "anthropic":
                return ConversionResult(success=True, data=data)
            elif source_format == "openai":
                return self._convert_from_openai_response(data)
            elif source_format == "gemini":
                return self._convert_from_gemini_response(data)
            else:
                return ConversionResult(
                    success=False,
                    error=f"Unsupported source format: {source_format}"
                )
        except Exception as e:
            self.logger.error(f"Failed to convert {source_format} response to Anthropic: {e}")
            return ConversionResult(success=False, error=str(e))
    
    def _convert_to_openai_request(self, data: Dict[str, Any]) -> ConversionResult:
        """è½¬æ¢Anthropicè¯·æ±‚åˆ°OpenAIæ ¼å¼"""
        result_data = {}
        
        # å¤„ç†æ¨¡å‹ - ç›´æ¥ä½¿ç”¨åŸå§‹æ¨¡å‹åï¼Œä¸è¿›è¡Œæ˜ å°„
        if "model" in data:
            result_data["model"] = data["model"]
        
        # å¤„ç†æ¶ˆæ¯å’Œç³»ç»Ÿæ¶ˆæ¯
        messages = []
        
        # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯
        if "system" in data:
            messages.append(self._create_system_message(data["system"]))
        
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        if "messages" in data:
            for msg in data["messages"]:
                role = msg.get("role")

                # ----------- æ™®é€š user / assistant æ–‡æœ¬ --------------
                if role in ["user", "assistant"]:
                    # æ£€æŸ¥æ˜¯å¦åŒ…å« tool_resultï¼ˆç”¨æˆ·æ¶ˆæ¯ä¸­çš„å·¥å…·ç»“æœï¼‰
                    if role == "user" and isinstance(msg.get("content"), list):
                        # æ£€æŸ¥æ˜¯å¦åŒ…å« tool_result
                        has_tool_result = any(
                            isinstance(item, dict) and item.get("type") == "tool_result"
                            for item in msg["content"]
                        )
                        
                        if has_tool_result:
                            # å¤„ç†ç”¨æˆ·çš„å·¥å…·ç»“æœæ¶ˆæ¯
                            for item in msg["content"]:
                                if isinstance(item, dict) and item.get("type") == "tool_result":
                                    tool_use_id = item.get("tool_use_id") or item.get("id") or ""
                                    content_str = str(item.get("content", ""))
                                    
                                    messages.append({
                                        "role": "tool",
                                        "tool_call_id": tool_use_id,
                                        "content": content_str,
                                    })
                            continue  # å·²å¤„ç†å·¥å…·ç»“æœï¼Œè·³è¿‡åç»­å¤„ç†
                    
                    # assistant ä¸”åŒ…å« tool_use â†’ function_call
                    if role == "assistant" and isinstance(msg.get("content"), list) and msg["content"]:
                        first_part = msg["content"][0]
                        if first_part.get("type") == "tool_use":
                            func_name = first_part.get("name", "")
                            func_args = first_part.get("input", {})
                            messages.append({
                                "role": "assistant",
                                "content": None,
                                "tool_calls": [
                                    {
                                        "id": first_part.get("id") or first_part.get("tool_use_id") or f"call_{func_name}_1",
                                        "type": "function",
                                        "function": {
                                            "name": func_name,
                                            "arguments": json.dumps(func_args, ensure_ascii=False)
                                        }
                                    }
                                ]
                            })
                            continue  # å·²å¤„ç†

                    # æ™®é€šæ–‡æœ¬æ¶ˆæ¯
                    content_converted = self._convert_content_from_anthropic(msg.get("content", ""))
                    
                    # è·³è¿‡ç©ºæ¶ˆæ¯ï¼Œé¿å…åœ¨å†å²ä¸­æ’å…¥ç©ºå­—ç¬¦ä¸²å¯¼è‡´æ¨¡å‹è¯¯åˆ¤
                    if not content_converted:
                        continue

                    messages.append({
                        "role": role,
                        "content": content_converted
                    })

                # å…¶å®ƒè§’è‰²å¿½ç•¥

        result_data["messages"] = messages

        # ---------------- OpenAI å…¼å®¹æ€§æ ¡éªŒ ----------------
        # ç¡®ä¿æ‰€æœ‰ assistant.tool_calls å‡æœ‰åç»­ tool å“åº”æ¶ˆæ¯ï¼›å¦åˆ™ç§»é™¤ä¸åŒ¹é…çš„ tool_call
        validated_messages: list = []
        for idx, m in enumerate(messages):
            if m.get("role") == "assistant" and m.get("tool_calls"):
                call_ids = [tc.get("id") for tc in m["tool_calls"] if tc.get("id")]
                # ç»Ÿè®¡åç»­æ˜¯å¦æœ‰å¯¹åº”çš„ tool æ¶ˆæ¯
                unmatched = set(call_ids)
                for later in messages[idx + 1:]:
                    if later.get("role") == "tool" and later.get("tool_call_id") in unmatched:
                        unmatched.discard(later["tool_call_id"])
                    if not unmatched:
                        break
                if unmatched:
                    # ç§»é™¤æ— åŒ¹é…çš„ tool_call
                    m["tool_calls"] = [tc for tc in m["tool_calls"] if tc.get("id") not in unmatched]
                    # å¦‚æœå…¨éƒ¨è¢«ç§»é™¤ï¼Œåˆ™é™çº§ä¸ºæ™®é€š assistant æ–‡æœ¬æ¶ˆæ¯
                    if not m["tool_calls"]:
                        m.pop("tool_calls", None)
                        if m.get("content") is None:
                            m["content"] = ""
            validated_messages.append(m)

        result_data["messages"] = validated_messages
        
        # å¤„ç†å…¶ä»–å‚æ•°
        if "max_tokens" in data:
            result_data["max_tokens"] = data["max_tokens"]
        if "temperature" in data:
            result_data["temperature"] = data["temperature"]
        if "top_p" in data:
            result_data["top_p"] = data["top_p"]
        if "top_k" in data:
            # OpenAIä¸æ”¯æŒtop_kï¼Œå¯ä»¥è®°å½•è­¦å‘Š
            pass
        if "stop_sequences" in data:
            result_data["stop"] = data["stop_sequences"]
        if "stream" in data:
            result_data["stream"] = data["stream"]
        
        # å¤„ç†å·¥å…·è°ƒç”¨
        if "tools" in data:
            openai_tools = []
            for tool in data["tools"]:
                openai_tools.append({
                    "type": "function",
                    "function": {
                        "name": tool.get("name", ""),
                        "description": tool.get("description", ""),
                        "parameters": self._clean_json_schema_properties(tool.get("input_schema", {}))
                    }
                })
            result_data["tools"] = openai_tools
            result_data["tool_choice"] = "auto"
        
        # å¤„ç†æ€è€ƒé¢„ç®—è½¬æ¢ (Anthropic thinking -> OpenAI reasoning_effort + max_completion_tokens)
        if "thinking" in data and data["thinking"].get("type") == "enabled":
            # æ£€æµ‹åˆ°æ€è€ƒå‚æ•°ï¼Œè®¾ç½®ä¸ºOpenAIæ€è€ƒæ¨¡å‹æ ¼å¼
            budget_tokens = data["thinking"].get("budget_tokens")
            
            # æ ¹æ®budget_tokensæ™ºèƒ½åˆ¤æ–­reasoning_effortç­‰çº§
            reasoning_effort = self._determine_reasoning_effort_from_budget(budget_tokens)
            result_data["reasoning_effort"] = reasoning_effort
            
            # å¤„ç†max_completion_tokensçš„ä¼˜å…ˆçº§é€»è¾‘
            max_completion_tokens = None
            
            # ä¼˜å…ˆçº§1ï¼šå®¢æˆ·ç«¯ä¼ å…¥çš„max_tokens
            if "max_tokens" in data:
                max_completion_tokens = data["max_tokens"]
                result_data.pop("max_tokens", None)  # ç§»é™¤max_tokensï¼Œä½¿ç”¨max_completion_tokens
                self.logger.info(f"Using client max_tokens as max_completion_tokens: {max_completion_tokens}")
            else:
                # ä¼˜å…ˆçº§2ï¼šç¯å¢ƒå˜é‡OPENAI_REASONING_MAX_TOKENS
                import os
                env_max_tokens = os.environ.get("OPENAI_REASONING_MAX_TOKENS")
                if env_max_tokens:
                    try:
                        max_completion_tokens = int(env_max_tokens)
                        self.logger.info(f"Using OPENAI_REASONING_MAX_TOKENS from environment: {max_completion_tokens}")
                    except ValueError:
                        self.logger.warning(f"Invalid OPENAI_REASONING_MAX_TOKENS value '{env_max_tokens}', must be integer")
                        env_max_tokens = None
                
                if not env_max_tokens:
                    # ä¼˜å…ˆçº§3ï¼šéƒ½æ²¡æœ‰åˆ™æŠ¥é”™
                    raise ConversionError("For OpenAI reasoning models, max_completion_tokens is required. Please specify max_tokens in the request or set OPENAI_REASONING_MAX_TOKENS environment variable.")
            
            result_data["max_completion_tokens"] = max_completion_tokens
            self.logger.info(f"Anthropic thinking enabled -> OpenAI reasoning_effort='{reasoning_effort}', max_completion_tokens={max_completion_tokens}")
            if budget_tokens:
                self.logger.info(f"Budget tokens: {budget_tokens} -> reasoning_effort: '{reasoning_effort}'")
        
        return ConversionResult(success=True, data=result_data)
    
    def _convert_to_gemini_request(self, data: Dict[str, Any]) -> ConversionResult:
        """è½¬æ¢Anthropicè¯·æ±‚åˆ°Geminiæ ¼å¼"""
        result_data = {}
        
        # å¤„ç†æ¨¡å‹åç§°
        if "model" in data:
            # ç›´æ¥ä½¿ç”¨åŸå§‹æ¨¡å‹åç§°ï¼Œä¸è¿›è¡Œæ˜ å°„
            result_data["model"] = data["model"]
        
        # å¤„ç†ç³»ç»Ÿæ¶ˆæ¯ - åŸºäº2025å¹´Gemini APIæ–‡æ¡£æ ¼å¼
        if "system" in data:
            # ç¡®ä¿ç³»ç»ŸæŒ‡ä»¤å†…å®¹ä¸ä¸ºç©ºä¸”ä¸ºå­—ç¬¦ä¸²
            system_content = str(data["system"]).strip() if data["system"] else ""
            if system_content:
                result_data["system_instruction"] = {
                    "parts": [{"text": system_content}]
                }
        
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        if "messages" in data:
            # æ„å»ºå·¥å…·è°ƒç”¨IDåˆ°å‡½æ•°åçš„æ˜ å°„
            tool_use_to_name = {}
            for msg in data["messages"]:
                if msg.get("role") == "assistant" and isinstance(msg.get("content"), list):
                    for item in msg["content"]:
                        if item.get("type") == "tool_use":
                            tool_use_to_name[item.get("id")] = item.get("name")
            
            # è®¾ç½®æ˜ å°„ä¾›_build_function_responseä½¿ç”¨
            self._tool_use_mapping = tool_use_to_name
            
            gemini_contents = []
            for msg in data["messages"]:
                parts_converted = self._convert_content_to_gemini(msg.get("content", ""))

                # -------- ä¿®æ­£è§’è‰²æ˜ å°„ï¼štool æ¶ˆæ¯ä¿æŒä¸º tool --------------
                anthropic_role = msg.get("role", "assistant")
                if anthropic_role == "user":
                    role = "user"
                elif anthropic_role == "assistant":
                    role = "model"
                elif anthropic_role == "tool":
                    role = "tool"
                else:
                    role = "model"

                # å¦‚æœå†…å®¹åŒ…å« functionResponseï¼Œå¼ºåˆ¶è®¾ä¸º tool è§’è‰²ï¼Œé¿å…ç”¨æˆ·ç«¯ role å†™é”™
                if any("functionResponse" in p for p in parts_converted):
                    role = "tool"
                
                # ç¡®ä¿ tool è§’è‰²çš„æ¶ˆæ¯è‡³å°‘æœ‰ä¸€ä¸ªæœ‰æ•ˆçš„ partï¼Œé¿å… Gemini 500 é”™è¯¯
                if role == "tool" and not parts_converted:
                    parts_converted = [{"text": ""}]
                elif role == "tool" and all(not p for p in parts_converted):
                    parts_converted = [{"text": ""}]
                
                gemini_contents.append({
                    "role": role,
                    "parts": parts_converted
                })
            result_data["contents"] = gemini_contents
        
        # å¤„ç†ç”Ÿæˆé…ç½®
        generation_config = {}
        if "temperature" in data:
            generation_config["temperature"] = data["temperature"]
        if "top_p" in data:
            generation_config["topP"] = data["top_p"]
        if "top_k" in data:
            generation_config["topK"] = data["top_k"]
        if "max_tokens" in data:
            generation_config["maxOutputTokens"] = data["max_tokens"]
        if "stop_sequences" in data:
            generation_config["stopSequences"] = data["stop_sequences"]
        
        # å¤„ç†æ€è€ƒé¢„ç®—è½¬æ¢ (Anthropic thinkingBudget -> Gemini thinkingBudget)
        if "thinking" in data and data["thinking"].get("type") == "enabled":
            budget_tokens = data["thinking"].get("budget_tokens")
            if budget_tokens:
                generation_config["thinkingConfig"] = {
                    "thinkingBudget": budget_tokens
                }
                self.logger.info(f"Anthropic thinkingBudget {budget_tokens} -> Gemini thinkingBudget {budget_tokens}")
            elif "thinking" in data:
                # å¦‚æœæ²¡æœ‰è®¾ç½®budget_tokensï¼Œå¯¹åº”Geminiçš„-1ï¼ˆåŠ¨æ€æ€è€ƒï¼‰
                generation_config["thinkingConfig"] = {
                    "thinkingBudget": -1
                }
                self.logger.info("Anthropic thinking enabled without budget -> Gemini thinkingBudget -1 (dynamic)")
        
        # ç¡®ä¿ generationConfig æ°¸è¿œå­˜åœ¨ï¼Œé¿å… Gemini 2.0+ çš„ 500 é”™è¯¯
        result_data["generationConfig"] = generation_config or {}
        
        # å¤„ç†å·¥å…·è°ƒç”¨
        if "tools" in data:
            function_declarations = []
            for tool in data["tools"]:
                function_declarations.append({
                    "name": tool.get("name", ""),
                    "description": tool.get("description", ""),
                    "parameters": self._clean_json_schema_properties(tool.get("input_schema", {}))
                })
            
            if function_declarations:
                # Geminiå®˜æ–¹è§„èŒƒä½¿ç”¨ camelCase: functionDeclarations
                result_data["tools"] = [{"functionDeclarations": function_declarations}]
        
        # åº”ç”¨æ·±åº¦æ¸…ç†ï¼Œç§»é™¤å¯èƒ½å¯¼è‡´åè®®é”™è¯¯çš„å­—æ®µ
        cleaned_result_data = self._deep_clean_for_gemini(result_data)
        
        return ConversionResult(success=True, data=cleaned_result_data)
    
    def _convert_from_openai_response(self, data: Dict[str, Any]) -> ConversionResult:
        """è½¬æ¢OpenAIå“åº”åˆ°Anthropicæ ¼å¼"""
        # å¿…é¡»æœ‰åŸå§‹æ¨¡å‹åç§°ï¼Œå¦åˆ™æŠ¥é”™
        if not self.original_model:
            raise ValueError("Original model name is required for response conversion")
            
        result_data = {
            "id": data.get("id", "msg_openai"),
            "type": "message",
            "role": "assistant",
            "content": [],
            "model": self.original_model,  # ä½¿ç”¨åŸå§‹æ¨¡å‹åç§°
            "stop_reason": "end_turn",
            "usage": {}
        }
        
        # å¤„ç†é€‰æ‹©
        if "choices" in data and data["choices"] and data["choices"][0]:
            choice = data["choices"][0]
            message = choice.get("message", {})
            # --------------- å¤„ç†å†…å®¹ä¸å·¥å…·è°ƒç”¨ ---------------
            content_list = []

            # 1) å¤„ç† tool_calls â†’ tool_use
            if message.get("tool_calls"):
                for tc in message["tool_calls"]:
                    if tc and "function" in tc:
                        func = tc["function"]
                    else:
                        continue
                    # OpenAI è§„èŒƒä¸­ arguments å¯èƒ½æ˜¯ JSON å­—ç¬¦ä¸²
                    arg_str = func.get("arguments", "{}")
                    try:
                        arg_obj = json.loads(arg_str) if isinstance(arg_str, str) else arg_str
                    except Exception:
                        arg_obj = {}

                    content_list.append({
                        "type": "tool_use",
                        "id": tc.get("id", ""),
                        "name": func.get("name", ""),
                        "input": arg_obj,
                    })

            # 2) æ™®é€šæ–‡æœ¬å†…å®¹ï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å«thinkingæ ‡ç­¾
            content_text = message.get("content", "")
            if content_text:
                # æ£€æŸ¥å¹¶æå–thinkingå†…å®¹
                extracted_content = self._extract_thinking_from_openai_text(content_text)
                if isinstance(extracted_content, list):
                    # æœ‰å¤šä¸ªå†…å®¹å—ï¼ˆåŒ…å«thinkingï¼‰
                    content_list.extend(extracted_content)
                else:
                    # æ™®é€šæ–‡æœ¬
                    content_list.append({
                        "type": "text",
                        "text": extracted_content,
                    })

            if content_list:
                result_data["content"] = content_list

            # æ˜ å°„ç»“æŸåŸå› ï¼ˆåŒ…å« tool_calls â†’ tool_useï¼‰
            finish_reason = choice.get("finish_reason", "stop")
            result_data["stop_reason"] = self._map_finish_reason(finish_reason, "openai", "anthropic")
        
        # å¤„ç†ä½¿ç”¨æƒ…å†µ
        if "usage" in data and data["usage"] is not None:
            usage = data["usage"]
            result_data["usage"] = {
                "input_tokens": usage.get("prompt_tokens", 0),
                "output_tokens": usage.get("completion_tokens", 0)
            }
        
        return ConversionResult(success=True, data=result_data)
    
    def _extract_thinking_from_openai_text(self, text: str) -> Any:
        """ä»OpenAIæ–‡æœ¬ä¸­æå–thinkingå†…å®¹ï¼Œè¿”å›Anthropicæ ¼å¼çš„content blocks"""
        import re
        
        # åŒ¹é… <thinking>...</thinking> æ ‡ç­¾
        thinking_pattern = r'<thinking>\s*(.*?)\s*</thinking>'
        matches = re.finditer(thinking_pattern, text, re.DOTALL)
        
        content_blocks = []
        last_end = 0
        
        for match in matches:
            # æ·»åŠ thinkingæ ‡ç­¾ä¹‹å‰çš„æ–‡æœ¬ï¼ˆå¦‚æœæœ‰ï¼‰
            before_text = text[last_end:match.start()].strip()
            if before_text:
                content_blocks.append({
                    "type": "text",
                    "text": before_text
                })
            
            # æ·»åŠ thinkingå†…å®¹
            thinking_text = match.group(1).strip()
            if thinking_text:
                content_blocks.append({
                    "type": "thinking",
                    "thinking": thinking_text
                })
            
            last_end = match.end()
        
        # æ·»åŠ æœ€åä¸€ä¸ªthinkingæ ‡ç­¾ä¹‹åçš„æ–‡æœ¬ï¼ˆå¦‚æœæœ‰ï¼‰
        after_text = text[last_end:].strip()
        if after_text:
            content_blocks.append({
                "type": "text",
                "text": after_text
            })
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°thinkingæ ‡ç­¾ï¼Œè¿”å›åŸæ–‡æœ¬
        if not content_blocks:
            return text
        
        # å¦‚æœåªæœ‰ä¸€ä¸ªæ–‡æœ¬å—ï¼Œè¿”å›å­—ç¬¦ä¸²
        if len(content_blocks) == 1 and content_blocks[0].get("type") == "text":
            return content_blocks[0]["text"]
        
        return content_blocks
    
    def _convert_from_gemini_response(self, data: Dict[str, Any]) -> ConversionResult:
        """è½¬æ¢Geminiå“åº”åˆ°Anthropicæ ¼å¼"""
        # å¿…é¡»æœ‰åŸå§‹æ¨¡å‹åç§°ï¼Œå¦åˆ™æŠ¥é”™
        if not self.original_model:
            raise ValueError("Original model name is required for response conversion")
            
        result_data = {
            "id": f"msg_gemini_{hash(str(data))}",
            "type": "message",
            "role": "assistant",
            "content": [],
            "model": self.original_model,  # ä½¿ç”¨åŸå§‹æ¨¡å‹åç§°
            "stop_reason": "end_turn",
            "usage": {}
        }
        
        # å¤„ç†å€™é€‰ç»“æœ
        if "candidates" in data and data["candidates"] and data["candidates"][0]:
            candidate = data["candidates"][0]
            content_list = []
            if "content" in candidate and "parts" in candidate["content"]:
                for part in candidate["content"]["parts"]:
                    # æ™®é€šæ–‡æœ¬
                    if "text" in part:
                        content_list.append({
                            "type": "text",
                            "text": part["text"]
                        })
                    # å‡½æ•°è°ƒç”¨
                    elif "functionCall" in part:
                        fc = part["functionCall"]
                        content_list.append({
                            "type": "tool_use",
                            "id": f"call_{fc.get('name','tool')}_{abs(hash(str(fc)))}",
                            "name": fc.get("name", ""),
                            "input": fc.get("args", {})
                        })
            if content_list:
                result_data["content"] = content_list

            # æ ¹æ®æ˜¯å¦å­˜åœ¨ functionCall åˆ¤æ–­ stop_reason
            finish_reason = candidate.get("finishReason", "STOP")
            if content_list and any(c.get("type") == "tool_use" for c in content_list):
                result_data["stop_reason"] = "tool_use"
            else:
                result_data["stop_reason"] = self._map_finish_reason(finish_reason, "gemini", "anthropic")
        
        # å¤„ç†ä½¿ç”¨æƒ…å†µ
        if "usageMetadata" in data:
            usage = data["usageMetadata"]
            result_data["usage"] = {
                "input_tokens": usage.get("promptTokenCount", 0),
                "output_tokens": usage.get("candidatesTokenCount", 0)
            }
        
        return ConversionResult(success=True, data=result_data)
    
    def _convert_from_openai_streaming_chunk(self, data: Dict[str, Any]) -> ConversionResult:
        """è½¬æ¢OpenAIæµå¼å“åº”chunkåˆ°Anthropic SSEæ ¼å¼ """
        import json, time, random
        
        # é¦–å…ˆéªŒè¯åŸå§‹æ¨¡å‹åç§°ï¼Œç¡®ä¿åœ¨çŠ¶æ€åˆå§‹åŒ–ä¹‹å‰å°±æ£€æŸ¥
        if not self.original_model:
            raise ValueError("Original model name is required for streaming response conversion")
        
        # åˆå§‹åŒ–æµçŠ¶æ€ 
        if not hasattr(self, '_streaming_state') or getattr(self, '_force_reset', False):
            # æ¸…ç†å¯èƒ½æ®‹ç•™çš„çŠ¶æ€ï¼Œé¿å…çŠ¶æ€æ±¡æŸ“
            for attr in ['_gemini_sent_start', '_gemini_stream_id', '_gemini_text_started', '_force_reset']:
                if hasattr(self, attr):
                    delattr(self, attr)
            
            self._streaming_state = {
                'message_id': f"msg_{int(time.time() * 1000)}",
                'model': self.original_model,  # ç¡®ä¿ä½¿ç”¨æœ‰æ•ˆçš„æ¨¡å‹åç§°
                'has_started': False,
                'has_text_content_started': False,
                'has_finished': False,
                'content_index': 0,
                'text_content_index': None,  # è®°å½•æ–‡æœ¬å—çš„ç´¢å¼•
                'tool_calls': {},  # Map[tool_call_index, ToolCallInfo]
                'tool_call_index_to_content_block_index': {},  # OpenAI index -> Anthropic index
                'is_closed': False
            }
        
        # è§£æOpenAI chunkæ•°æ®
        choice = None
        if "choices" in data and data["choices"] and data["choices"][0]:
            choice = data["choices"][0]
        
        if not choice:
            return ConversionResult(success=True, data="")
        
        delta = choice.get("delta", {})
        content = delta.get("content", "")
        tool_calls = delta.get("tool_calls", [])
        finish_reason = choice.get("finish_reason")
        
        events = []
        state = self._streaming_state
        
        # 1. å‘é€message_start (ä»…ä¸€æ¬¡ï¼Œåœ¨ç¬¬ä¸€æ¬¡æ”¶åˆ°chunkæ—¶)
        # ä¿®å¤ï¼šåº”è¯¥åœ¨æ”¶åˆ°ä»»ä½•è§’è‰²ä¿¡æ¯æˆ–æœ‰æ„ä¹‰çš„deltaæ—¶å°±å‘é€ï¼Œè€Œä¸åªæ˜¯ç­‰å¾…contentæˆ–tool_calls
        if not state['has_started'] and not state['is_closed']:
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ„ä¹‰çš„chunkï¼ˆæœ‰roleã€contentæˆ–tool_callsï¼‰
            has_meaningful_data = (
                delta.get("role") or 
                content or 
                tool_calls or 
                # æˆ–è€…choiceä¸­æœ‰ä»»ä½•æœ‰æ„ä¹‰çš„ä¿¡æ¯
                choice.get("index") is not None
            )
            
            if has_meaningful_data:
                state['has_started'] = True
                # ç¡®ä¿ä½¿ç”¨æœ‰æ•ˆçš„æ¨¡å‹åç§°å’Œrole - æ·»åŠ é˜²å¾¡æ€§æ£€æŸ¥
                model_name = state.get('model') or self.original_model or 'unknown'
                message_start = {
                    "type": "message_start",
                    "message": {
                        "id": state['message_id'],
                        "type": "message",
                        "role": "assistant",  # å§‹ç»ˆç¡®ä¿roleå±æ€§å­˜åœ¨
                        "content": [],
                        "model": model_name,  # ä½¿ç”¨é˜²å¾¡æ€§æ£€æŸ¥åçš„æ¨¡å‹åç§°
                        "stop_reason": None,
                        "stop_sequence": None,
                        "usage": {"input_tokens": 0, "output_tokens": 0}
                    }
                }
                events.append(f"event: message_start\ndata: {json.dumps(message_start, ensure_ascii=False)}\n\n")
        
        # 2. å¤„ç†æ–‡æœ¬å†…å®¹
        if content and not state['is_closed']:
            # å¼€å§‹æ–‡æœ¬content block
            if not state['has_text_content_started']:
                state['has_text_content_started'] = True
                state['text_content_index'] = state['content_index']  # è®°ä½æ–‡æœ¬å—çš„ç´¢å¼•
                content_block_start = {
                    "type": "content_block_start",
                    "index": state['text_content_index'],
                    "content_block": {"type": "text", "text": ""}
                }
                events.append(f"event: content_block_start\ndata: {json.dumps(content_block_start, ensure_ascii=False)}\n\n")
                state['content_index'] += 1  # ä¸ºåç»­å—é€’å¢ç´¢å¼•
            
            # å‘é€æ–‡æœ¬å¢é‡
            content_delta = {
                "type": "content_block_delta",
                "index": state['text_content_index'],
                "delta": {
                    "type": "text_delta",
                    "text": content
                }
            }
            events.append(f"event: content_block_delta\ndata: {json.dumps(content_delta, ensure_ascii=False)}\n\n")
        
        # 3. å¤„ç†å·¥å…·è°ƒç”¨ - å…³é”®éƒ¨åˆ†
        if tool_calls and not state['is_closed']:
            processed_in_this_chunk = set()
            
            for tool_call in tool_calls:
                if not tool_call:
                    continue
                
                tool_call_index = tool_call.get('index', 0)
                if tool_call_index in processed_in_this_chunk:
                    continue
                processed_in_this_chunk.add(tool_call_index)
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°çš„tool_call_index
                is_unknown_index = tool_call_index not in state['tool_call_index_to_content_block_index']
                
                if is_unknown_index:
                    # ä¸ºæ–°çš„å·¥å…·è°ƒç”¨åˆ†é…content_blockç´¢å¼•
                    tool_content_block_index = state['content_index']
                    state['tool_call_index_to_content_block_index'][tool_call_index] = tool_content_block_index
                    
                    # ç”Ÿæˆå·¥å…·è°ƒç”¨IDå’Œåç§°
                    tool_call_id = tool_call.get('id', f"call_{int(time.time())}_{tool_call_index}")
                    tool_call_name = tool_call.get('function', {}).get('name', f"tool_{tool_call_index}")
                    
                    # å¼€å§‹æ–°çš„tool_use content block
                    content_block_start = {
                        "type": "content_block_start",
                        "index": tool_content_block_index,
                        "content_block": {
                            "type": "tool_use",
                            "id": tool_call_id,
                            "name": tool_call_name,
                            "input": {}
                        }
                    }
                    events.append(f"event: content_block_start\ndata: {json.dumps(content_block_start, ensure_ascii=False)}\n\n")
                    
                    # å­˜å‚¨å·¥å…·è°ƒç”¨ä¿¡æ¯
                    state['tool_calls'][tool_call_index] = {
                        'id': tool_call_id,
                        'name': tool_call_name,
                        'arguments': '',
                        'content_block_index': tool_content_block_index
                    }
                    
                    state['content_index'] += 1  # ä¸ºä¸‹ä¸€ä¸ªå—é€’å¢ç´¢å¼•
                
                # å¤„ç†å‡½æ•°å‚æ•°ç´¯ç§¯
                if tool_call.get('function', {}).get('arguments'):
                    current_tool_call = state['tool_calls'].get(tool_call_index)
                    if current_tool_call:
                        # ç´¯ç§¯å‚æ•°ç‰‡æ®µ
                        arguments_fragment = tool_call['function']['arguments']
                        current_tool_call['arguments'] += arguments_fragment
                        
                        # æ¸…ç†å¹¶éªŒè¯JSONç‰‡æ®µ
                        cleaned_fragment = self._clean_json_fragment(arguments_fragment)
                        
                        if cleaned_fragment:  # åªå‘é€éç©ºçš„æ¸…ç†åç‰‡æ®µ
                            # å‘é€å‚æ•°å¢é‡ï¼ˆä½¿ç”¨æ­£ç¡®çš„content_blockç´¢å¼•ï¼‰
                            input_json_delta = {
                                "type": "content_block_delta",
                                "index": current_tool_call['content_block_index'],
                                "delta": {
                                    "type": "input_json_delta",
                                    "partial_json": cleaned_fragment
                                }
                            }
                            events.append(f"event: content_block_delta\ndata: {json.dumps(input_json_delta, ensure_ascii=False)}\n\n")
        
        # 4. å¤„ç†æµç»“æŸ - åªæœ‰åœ¨messageå·²ç»å¼€å§‹çš„æƒ…å†µä¸‹æ‰å¤„ç†
        if finish_reason and not state['has_finished'] and state['has_started']:
            state['has_finished'] = True
            
            # åœæ­¢æ‰€æœ‰æ´»è·ƒçš„content blocks
            # å…ˆåœæ­¢æ‰€æœ‰å·¥å…·è°ƒç”¨å—
            for tool_call_info in state['tool_calls'].values():
                content_block_stop = {"type": "content_block_stop", "index": tool_call_info['content_block_index']}
                events.append(f"event: content_block_stop\ndata: {json.dumps(content_block_stop, ensure_ascii=False)}\n\n")
            
            # åœæ­¢æ–‡æœ¬å—ï¼ˆå¦‚æœæœ‰ï¼‰
            if state['has_text_content_started'] and not state['is_closed']:
                content_block_stop = {"type": "content_block_stop", "index": state['text_content_index']}
                events.append(f"event: content_block_stop\ndata: {json.dumps(content_block_stop, ensure_ascii=False)}\n\n")
            
            # æ˜ å°„finish_reason - ä½¿ç”¨ç»Ÿä¸€çš„æ˜ å°„æ–¹æ³•
            anthropic_stop_reason = self._map_finish_reason(finish_reason, "openai", "anthropic")
            
            # å‘é€message_delta
            message_delta = {
                "type": "message_delta",
                "delta": {
                    "stop_reason": anthropic_stop_reason,
                    "stop_sequence": None
                }
            }
            
            # æ€»æ˜¯æä¾› usage ä¿¡æ¯ï¼Œå³ä½¿ OpenAI è¿”å› null
            if "usage" in data and data["usage"] is not None:
                usage = data["usage"]
                message_delta["usage"] = {
                    "input_tokens": usage.get("prompt_tokens", 0),
                    "output_tokens": usage.get("completion_tokens", 0)
                }
            else:
                # å¦‚æœæ²¡æœ‰ usage ä¿¡æ¯ï¼Œæä¾›é»˜è®¤å€¼ä»¥é¿å…å‰ç«¯é”™è¯¯
                message_delta["usage"] = {
                    "input_tokens": 0,
                    "output_tokens": 0
                }
            
            events.append(f"event: message_delta\ndata: {json.dumps(message_delta, ensure_ascii=False)}\n\n")
            
            # å‘é€message_stop
            message_stop = {"type": "message_stop"}
            events.append(f"event: message_stop\ndata: {json.dumps(message_stop, ensure_ascii=False)}\n\n")
        
        # æ¸…ç†çŠ¶æ€ï¼ˆå¦‚æœæµç»“æŸäº†ï¼‰
        if finish_reason:
            state['is_closed'] = True
            if hasattr(self, '_streaming_state'):
                delattr(self, '_streaming_state')
        
        # è¿”å›ç»“æœ - æ”¹è¿›äº‹ä»¶å¤„ç†é€»è¾‘
        if not events:
            # å³ä½¿æ²¡æœ‰äº‹ä»¶ï¼Œä¹Ÿè¦è®°å½•è¿™ç§æƒ…å†µä»¥ä¾¿è°ƒè¯•
            self.logger.debug(f"No events generated for chunk - content: {bool(content)}, tool_calls: {bool(tool_calls)}, has_started: {state.get('has_started', False)}")
            return ConversionResult(success=True, data="")
        
        result_data = "".join(events)
        self.logger.debug(f"Generated {len(events)} events, total data length: {len(result_data)}")
        return ConversionResult(success=True, data=result_data)
    
    def _clean_json_fragment(self, fragment: str) -> str:
        """æ¸…ç†JSONç‰‡æ®µï¼Œé¿å…ä¸å®Œæ•´çš„Unicodeå­—ç¬¦æˆ–è½¬ä¹‰åºåˆ—"""
        if not fragment:
            return fragment
        
        try:
            # ç§»é™¤å¼€å¤´å’Œç»“å°¾å¯èƒ½ä¸å®Œæ•´çš„Unicodeå­—ç¬¦
            # æ£€æŸ¥æœ€åå‡ ä¸ªå­—ç¬¦æ˜¯å¦æ˜¯ä¸å®Œæ•´çš„è½¬ä¹‰åºåˆ—
            cleaned = fragment
            
            # å¤„ç†å¯èƒ½è¢«æˆªæ–­çš„è½¬ä¹‰åºåˆ—
            if cleaned.endswith('\\') and not cleaned.endswith('\\\\'):
                cleaned = cleaned[:-1]  # ç§»é™¤æ‚¬æŒ‚çš„åæ–œæ 
            elif cleaned.endswith('\\u') or cleaned.endswith('\\u0') or cleaned.endswith('\\u00'):
                # ä¸å®Œæ•´çš„Unicodeè½¬ä¹‰åºåˆ—
                idx = cleaned.rfind('\\u')
                cleaned = cleaned[:idx]
            
            # éªŒè¯æ¸…ç†åçš„ç‰‡æ®µä¸ä¼šå¯¼è‡´JSONè§£æé”™è¯¯
            if cleaned:
                # ç®€å•æµ‹è¯•ï¼šå¦‚æœç‰‡æ®µåŒ…å«å¼•å·ï¼Œç¡®ä¿å®ƒä»¬æ˜¯å¹³è¡¡çš„
                quote_count = cleaned.count('"') - cleaned.count('\\"')
                if quote_count % 2 == 1:
                    # å¦‚æœå¼•å·æ•°é‡ä¸ºå¥‡æ•°ï¼Œå¯èƒ½åœ¨å­—ç¬¦ä¸²ä¸­é—´è¢«æˆªæ–­
                    pass  # ä»ç„¶å‘é€ï¼Œè®©æ¥æ”¶ç«¯å¤„ç†
            
            return cleaned
            
        except Exception as e:
            self.logger.warning(f"Error cleaning JSON fragment: {e}, returning original")
            return fragment
    
    
    def _convert_from_gemini_streaming_chunk(self, data: Dict[str, Any]) -> ConversionResult:
        """å°† Gemini æµå¼ chunk è½¬ä¸º Anthropic SSE æ ¼å¼ - ç®€åŒ–ç‰ˆæœ¬"""
        import json, random, time
        
        self.logger.debug(f"Converting Gemini chunk: {str(data)[:200]}...")
        
        # æ£€æŸ¥å½“å‰çŠ¶æ€
        current_state = {
            '_gemini_stream_id': hasattr(self, '_gemini_stream_id'),
            '_gemini_sent_start': hasattr(self, '_gemini_sent_start'),
            '_gemini_text_started': hasattr(self, '_gemini_text_started'),
            '_streaming_state': hasattr(self, '_streaming_state'),
            '_force_reset': getattr(self, '_force_reset', False)
        }
        self.logger.debug(f"Current state before processing: {current_state}")
        
        # æ¯æ¬¡å¼€å§‹æ–°çš„æµå¼è½¬æ¢æ—¶ï¼Œé‡ç½®æ‰€æœ‰ç›¸å…³çŠ¶æ€å˜é‡ï¼Œé¿å…çŠ¶æ€æ±¡æŸ“
        if not hasattr(self, '_gemini_stream_id') or getattr(self, '_force_reset', False):
            self.logger.debug("Initializing new Gemini stream")
            # æ¸…ç†å¯èƒ½æ®‹ç•™çš„çŠ¶æ€
            for attr in ['_gemini_sent_start', '_gemini_text_started', '_streaming_state', '_force_reset']:
                if hasattr(self, attr):
                    delattr(self, attr)
            # ç”Ÿæˆæ–°çš„æµID
            self._gemini_stream_id = f"msg_{random.randint(100000, 999999)}"
            self.logger.debug(f"Generated stream ID: {self._gemini_stream_id}")

        # ä¿å­˜æ¨¡å‹åï¼ˆå¿…é¡»å·²åœ¨ set_original_model è®¾ç½®ï¼‰
        if not self.original_model:
            raise ValueError("Original model name is required for streaming response conversion")

        # æå–æœ¬æ¬¡ chunk çš„ candidateã€å†…å®¹ã€ç»“æŸæ ‡è®°
        candidate = None
        if data.get("candidates") and data["candidates"][0]:
            candidate = data["candidates"][0]

        content = ""
        function_calls = []
        if candidate and candidate.get("content") and candidate["content"].get("parts"):
            for part in candidate["content"]["parts"]:
                if "text" in part:
                    content += part["text"]
                elif "functionCall" in part:
                    # å¤„ç†å‡½æ•°è°ƒç”¨
                    func_call = part["functionCall"]
                    function_calls.append({
                        "name": func_call.get("name", ""),
                        "args": func_call.get("args", {})
                    })

        is_end = bool(candidate and candidate.get("finishReason"))

        events: list[str] = []  # ä¿å­˜ SSE è¡Œ

        # ç¬¬ä¸€æ¬¡è¿›å…¥ï¼šå‘é€ message_start
        if not hasattr(self, '_gemini_sent_start'):
            import logging
            unified_logger = logging.getLogger("unified_api")
            unified_logger.debug("ANTHROPIC_CONVERTER: Sending message_start for new Gemini stream")
            self.logger.debug("Sending message_start for new Gemini stream")
            self._gemini_sent_start = True

            # é˜²å¾¡æ€§æ£€æŸ¥ï¼šç¡®ä¿modelå’Œroleå±æ€§å§‹ç»ˆæœ‰æ•ˆ
            model_name = self.original_model or 'unknown'
            message_start = {
                "type": "message_start",
                "message": {
                    "id": self._gemini_stream_id,
                    "type": "message",
                    "role": "assistant",  # å§‹ç»ˆç¡®ä¿roleå±æ€§å­˜åœ¨
                    "content": [],
                    "model": model_name,  # ä½¿ç”¨é˜²å¾¡æ€§æ£€æŸ¥åçš„æ¨¡å‹åç§°
                    "stop_reason": None,
                    "stop_sequence": None,
                    "usage": {"input_tokens": 0, "output_tokens": 0},
                },
            }
            events += [
                "event: message_start",
                f"data: {json.dumps(message_start, ensure_ascii=False)}",
                "",
            ]

        # å¤„ç†æ–‡æœ¬å†…å®¹
        if content:
            # å¦‚æœè¿˜æ²¡æœ‰å‘é€è¿‡æ–‡æœ¬ content_block_startï¼Œå…ˆå‘é€
            if not hasattr(self, '_gemini_text_started'):
                self._gemini_text_started = True
                content_block_start = {
                    "type": "content_block_start",
                    "index": 0,
                    "content_block": {"type": "text", "text": ""},
                }
                events += [
                    "event: content_block_start",
                    f"data: {json.dumps(content_block_start, ensure_ascii=False)}",
                    "",
                ]

            # å‘é€æ–‡æœ¬å¢é‡
            content_block_delta = {
                "type": "content_block_delta",
                "index": 0,
                "delta": {"type": "text_delta", "text": content},
            }
            events += [
                "event: content_block_delta",
                f"data: {json.dumps(content_block_delta, ensure_ascii=False)}",
                "",
            ]

        # å¤„ç†å‡½æ•°è°ƒç”¨
        if function_calls:
            for i, func_call in enumerate(function_calls):
                # ç¡®å®šç´¢å¼•ï¼šå¦‚æœæœ‰æ–‡æœ¬å†…å®¹ï¼Œå·¥å…·è°ƒç”¨ä»ç´¢å¼•1å¼€å§‹ï¼›å¦åˆ™ä»ç´¢å¼•0å¼€å§‹
                # ç”±äºæˆ‘ä»¬ä¸Šé¢å¯èƒ½æ·»åŠ äº†è§£é‡Šæ–‡æœ¬ï¼Œæ‰€ä»¥_gemini_text_startedåº”è¯¥å·²ç»è®¾ç½®
                tool_index = 1 if hasattr(self, '_gemini_text_started') else 0
                # å¦‚æœæœ‰å¤šä¸ªå·¥å…·è°ƒç”¨ï¼Œåç»­å·¥å…·çš„ç´¢å¼•éœ€è¦é€’å¢
                tool_index += i
                
                # å‘é€ tool_use content_block_start
                tool_block_start = {
                    "type": "content_block_start",
                    "index": tool_index,
                    "content_block": {
                        "type": "tool_use",
                        "id": f"toolu_{random.randint(100000, 999999)}",
                        "name": func_call["name"],
                        "input": {}
                    }
                }
                events += [
                    "event: content_block_start",
                    f"data: {json.dumps(tool_block_start, ensure_ascii=False)}",
                    "",
                ]

                # å‘é€å·¥å…·è°ƒç”¨å‚æ•°
                if func_call["args"]:
                    tool_delta = {
                        "type": "content_block_delta",
                        "index": tool_index,
                        "delta": {
                            "type": "input_json_delta",
                            "partial_json": json.dumps(func_call["args"], ensure_ascii=False)
                        }
                    }
                    events += [
                        "event: content_block_delta",
                        f"data: {json.dumps(tool_delta, ensure_ascii=False)}",
                        "",
                    ]

                # å‘é€ content_block_stop
                tool_block_stop = {"type": "content_block_stop", "index": tool_index}
                events += [
                    "event: content_block_stop",
                    f"data: {json.dumps(tool_block_stop, ensure_ascii=False)}",
                    "",
                ]

        # å¦‚æœæœ¬ chunk æºå¸¦ finishReasonï¼Œè¯´æ˜å¯¹è¯ç»“æŸï¼Œè¡¥å……æ”¶å°¾äº‹ä»¶
        if is_end:
            self.logger.debug(f"Stream ending with finishReason: {candidate.get('finishReason') if candidate else 'None'}")
            # å¦‚æœæœ‰æ–‡æœ¬å†…å®¹å—è¿˜æœªç»“æŸï¼Œå‘é€ content_block_stop
            if hasattr(self, '_gemini_text_started'):
                content_block_stop = {"type": "content_block_stop", "index": 0}
                events += [
                    "event: content_block_stop",
                    f"data: {json.dumps(content_block_stop, ensure_ascii=False)}",
                    "",
                ]

            # message_deltaï¼ˆåŒ…å« stop_reason ä¸ usageï¼‰
            # å¯¹äºGeminiå·¥å…·è°ƒç”¨çš„ç‰¹æ®Šå¤„ç†ï¼š
            # - å¦‚æœæ£€æµ‹åˆ°å‡½æ•°è°ƒç”¨ï¼Œstop_reasonåº”è¯¥æ˜¯tool_useï¼ˆæ— è®ºGeminiçš„finishReasonæ˜¯ä»€ä¹ˆï¼‰
            # - å¦‚æœæ²¡æœ‰å‡½æ•°è°ƒç”¨ï¼Œä½¿ç”¨æ­£å¸¸çš„finish_reasonæ˜ å°„
            if function_calls:
                stop_reason = "tool_use"
                self.logger.info(f"Setting stop_reason to 'tool_use' due to detected function calls: {[fc.get('name') for fc in function_calls]}")
            else:
                stop_reason = self._map_finish_reason(candidate.get("finishReason", ""), "gemini", "anthropic")
                self.logger.debug(f"Mapped finish_reason '{candidate.get('finishReason', '')}' to '{stop_reason}'")
            
            message_delta = {
                "type": "message_delta",
                "delta": {
                    "stop_reason": stop_reason,
                    "stop_sequence": None,
                },
            }
            
            # æ€»æ˜¯æä¾› usage ä¿¡æ¯ï¼Œå³ä½¿ Gemini æ²¡æœ‰ usageMetadata
            if data.get("usageMetadata"):
                usage = data["usageMetadata"]
                message_delta["usage"] = {
                    "input_tokens": usage.get("promptTokenCount", 0),
                    "output_tokens": usage.get("candidatesTokenCount", 0)
                }
            else:
                # å¦‚æœæ²¡æœ‰ usage ä¿¡æ¯ï¼Œæä¾›é»˜è®¤å€¼ä»¥é¿å…å‰ç«¯é”™è¯¯
                message_delta["usage"] = {
                    "input_tokens": 0,
                    "output_tokens": 0
                }

            events += [
                "event: message_delta",
                f"data: {json.dumps(message_delta, ensure_ascii=False)}",
                "",
                "event: message_stop",
                "data: {\"type\": \"message_stop\"}",
                "",
            ]
            import logging
            unified_logger = logging.getLogger("unified_api")
            unified_logger.debug("ANTHROPIC_CONVERTER: Sent message_stop for Gemini stream")
            self.logger.debug("Sent message_stop for Gemini stream")

            # ç»“æŸå½“å‰æµåæ¸…ç†çŠ¶æ€ï¼Œé¿å…å½±å“ä¸‹ä¸€æ¬¡è¯·æ±‚
            self.logger.debug("Cleaning up Gemini streaming state after stream end")
            cleaned_attrs = []
            if hasattr(self, '_gemini_sent_start'):
                cleaned_attrs.append('_gemini_sent_start')
                delattr(self, '_gemini_sent_start')
            if hasattr(self, '_gemini_stream_id'):
                cleaned_attrs.append('_gemini_stream_id')
                delattr(self, '_gemini_stream_id')
            if hasattr(self, '_gemini_text_started'):
                cleaned_attrs.append('_gemini_text_started')
                delattr(self, '_gemini_text_started')
            self.logger.debug(f"Cleaned up attributes after stream end: {cleaned_attrs}")

        # è‹¥æ²¡æœ‰ä»»ä½•äº‹ä»¶éœ€è¦å‘é€ï¼Œåˆ™è¿”å›ç©ºå­—ç¬¦ä¸²ï¼ˆä¸Šå±‚ä¼šå¿½ç•¥ï¼‰
        if not events:
            return ConversionResult(success=True, data="")

        # å°†äº‹ä»¶æŒ‰ "\n\n" åˆ†ç»„ï¼Œæ¯ä¸ªå®Œæ•´äº‹ä»¶ä½œä¸ºåˆ—è¡¨çš„ä¸€ä¸ªå…ƒç´ 
        complete_events = []
        i = 0
        while i < len(events):
            if events[i].startswith("event:") or events[i].startswith("data:"):
                # æ‰¾åˆ°ä¸€ä¸ªå®Œæ•´äº‹ä»¶çš„ç»“æŸï¼ˆä¸‹ä¸€ä¸ªç©ºè¡Œï¼‰
                event_lines = []
                while i < len(events) and events[i] != "":
                    event_lines.append(events[i])
                    i += 1
                # æ·»åŠ ç»“æŸçš„ç©ºè¡Œ
                if i < len(events) and events[i] == "":
                    event_lines.append("")
                    i += 1
                # å°†å®Œæ•´äº‹ä»¶æ‹¼æ¥æˆå­—ç¬¦ä¸²
                complete_events.append("\n".join(event_lines) + "\n")
            else:
                i += 1

        self.logger.debug(f"Successfully converted Gemini chunk to {len(complete_events)} events")
        return ConversionResult(success=True, data=complete_events)
        
    
    def _parse_anthropic_sse_event(self, sse_data: str) -> ConversionResult:
        """è§£æAnthropic SSEäº‹ä»¶æ•°æ®ï¼Œæå–äº‹ä»¶ç±»å‹å’Œæ•°æ®
        
        """
        import re
        import json
        
        # ä½¿ç”¨ä¸claude-to-chatgpté¡¹ç›®ç›¸åŒçš„æ­£åˆ™è¡¨è¾¾å¼
        # /event:\s*.*?\s*\ndata:\s*(.*?)(?=\n\n|\s*$)/gs
        pattern = r'event:\s*([^\n]*)\s*\ndata:\s*([^\n]*)'
        matches = re.findall(pattern, sse_data)
        
        parsed_events = []
        for event_type, data_content in matches:
            event_type = event_type.strip()
            data_content = data_content.strip()
            
            # å°è¯•è§£æJSONæ•°æ®
            try:
                if data_content:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ç»“æŸæ ‡è®°
                    if data_content.strip() == "[DONE]":
                        break
                    parsed_data = json.loads(data_content)
                    parsed_events.append({
                        'event': event_type,
                        'data': parsed_data
                    })
            except json.JSONDecodeError:
                # å¦‚æœä¸æ˜¯JSONï¼Œè·³è¿‡æˆ–è®°å½•è­¦å‘Š
                self.logger.warning(f"Failed to parse SSE data as JSON: {data_content}")
                continue
        
        return ConversionResult(success=True, data=parsed_events)
    
    def _convert_content_from_anthropic(self, content: Any) -> Any:
        """è½¬æ¢Anthropicå†…å®¹åˆ°é€šç”¨æ ¼å¼"""
        if isinstance(content, str):
            return content
        elif isinstance(content, list):
            # å¤„ç†å¤šæ¨¡æ€å†…å®¹
            converted_content = []
            for item in content:
                if item.get("type") == "text":
                    converted_content.append({
                        "type": "text",
                        "text": item.get("text", "")
                    })
                elif item.get("type") == "image":
                    # è½¬æ¢å›¾åƒæ ¼å¼
                    source = item.get("source", {})
                    if source.get("type") == "base64":
                        media_type = source.get("media_type", "image/jpeg")
                        data_part = source.get("data", "")
                        converted_content.append({
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:{media_type};base64,{data_part}"
                            }
                        })
            return converted_content if len(converted_content) > 1 else converted_content[0].get("text", "") if converted_content else ""
        return content
    
    def _convert_content_to_gemini(self, content: Any) -> List[Dict[str, Any]]:
        """å°† Anthropic çš„ content è½¬ä¸º Gemini parts ç»“æ„"""

        # 1. çº¯æ–‡æœ¬
        if isinstance(content, str):
            return [{"text": content}]

        # 2. åˆ—è¡¨ï¼ˆå¯èƒ½æ··æ‚å¤šæ¨¡æ€ / tool æ¶ˆæ¯ï¼‰
        if isinstance(content, list):
            gemini_parts: List[Dict[str, Any]] = []
            for item in content:
                if not isinstance(item, dict):
                    continue
                    
                item_type = item.get("type")

                # 2.1 æ™®é€šæ–‡æœ¬
                if item_type == "text":
                    text_content = item.get("text", "")
                    if text_content:  # åªæ·»åŠ éç©ºæ–‡æœ¬
                        gemini_parts.append({"text": text_content})

                # 2.2 å›¾åƒï¼ˆbase64ï¼‰
                elif item_type == "image":
                    source = item.get("source", {})
                    if source.get("type") == "base64":
                        gemini_parts.append({
                            "inlineData": {
                                "mimeType": source.get("media_type", "image/jpeg"),
                                "data": source.get("data", "")
                            }
                        })

                # 2.3 tool_use â†’ functionCall
                elif item_type == "tool_use":
                    tool_name = item.get("name", "")
                    tool_id = item.get("id", "")
                    
                    # å­˜å‚¨tool_idåˆ°function_nameçš„æ˜ å°„ï¼Œç”¨äºåç»­tool_resultè½¬æ¢
                    if tool_id and tool_name:
                        self._tool_id_mapping[tool_name] = tool_id
                        # åŒæ—¶å­˜å‚¨åˆ°å…¨å±€å·¥å…·çŠ¶æ€ç®¡ç†å™¨ä¸­
                        tool_state_manager.store_tool_mapping(tool_name, tool_id)
                        
                    gemini_parts.append({
                        "functionCall": {
                            "name": tool_name,
                            "args": item.get("input", {})
                        }
                    })

                # 2.4 tool_result â†’ functionResponse
                elif item_type == "tool_result":
                    fr = self._build_function_response(item)
                    if fr:
                        gemini_parts.append(fr)

            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„ partsï¼Œè¿”å›ç©ºæ–‡æœ¬è€Œä¸æ˜¯ç©ºæ•°ç»„
            if not gemini_parts:
                return [{"text": ""}]
            
            return gemini_parts

        # 3. å•ä¸ª dictï¼ˆå¯èƒ½å°±æ˜¯ tool_resultï¼‰
        if isinstance(content, dict):
            fr = self._build_function_response(content)
            if fr:
                return [fr]
            # å¦‚æœä¸æ˜¯å·¥å…·ç»“æœï¼Œè½¬ä¸ºæ–‡æœ¬
            content_text = content.get("text") or json.dumps(content, ensure_ascii=False)
            return [{"text": content_text}]

        # 4. å…¶å®ƒç±»å‹ç»Ÿä¸€è½¬å­—ç¬¦ä¸²
        return [{"text": str(content) if content else ""}]

    # --------- è¾…åŠ©ï¼šæ„é€  functionResponse part ---------
    def _build_function_response(self, item: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """æ ¹æ® tool_result å­—æ®µæ„é€  Gemini functionResponse"""
        if not isinstance(item, dict):
            return None

        # åˆ¤å®šæ˜¯å¦ä¸ºå·¥å…·ç»“æœ
        is_result = (
            item.get("type") == "tool_result"
            or "tool_use_id" in item
            or "tool_output" in item
            or "result" in item
            or "content" in item
        )
        if not is_result:
            return None

        # æå–å‡½æ•°å
        func_name = None
        
        # æ–¹æ³•1ï¼šä»æ˜ å°„è¡¨ä¸­è·å–ï¼ˆAnthropicæ ¼å¼ï¼‰
        tool_use_id = item.get("tool_use_id") or item.get("id")
        if tool_use_id and hasattr(self, '_tool_use_mapping'):
            func_name = self._tool_use_mapping.get(tool_use_id)
        
        # æ–¹æ³•1.5ï¼šä½¿ç”¨å…¨å±€å·¥å…·çŠ¶æ€ç®¡ç†å™¨
        if not func_name and tool_use_id:
            # å…ˆå°è¯•ä»IDä¸­æå–å¯èƒ½çš„å‡½æ•°å
            potential_func_name = None
            if str(tool_use_id).startswith("call_"):
                name_and_hash = tool_use_id[len("call_"):]
                potential_func_name = name_and_hash.rsplit("_", 1)[0]
            
            # æ£€æŸ¥å…¨å±€ç®¡ç†å™¨ä¸­æ˜¯å¦æœ‰å¯¹åº”çš„æ˜ å°„
            if potential_func_name:
                stored_id = tool_state_manager.get_tool_id(potential_func_name)
                if stored_id == tool_use_id:
                    func_name = potential_func_name
        
        # æ–¹æ³•2ï¼šä» tool_use_id ä¸­æå–ï¼ˆOpenAIæ ¼å¼ï¼‰
        if not func_name and tool_use_id and str(tool_use_id).startswith("call_"):
            # æ ¼å¼: call_<function_name>_<hash> ï¼Œå‡½æ•°åå¯èƒ½åŒ…å«å¤šä¸ªä¸‹åˆ’çº¿
            name_and_hash = tool_use_id[len("call_"):]
            func_name = name_and_hash.rsplit("_", 1)[0]  # å»æ‰æœ€åä¸€ä¸ª hash æ®µ
        
        # æ–¹æ³•3ï¼šç›´æ¥ä»å­—æ®µè·å–
        if not func_name:
            func_name = (
                item.get("tool_name")
                or item.get("name")
                or item.get("function_name")
            )

        if not func_name:
            return None

        # æå–ç»“æœå†…å®¹
        func_response = None
        
        # å°è¯•å¤šä¸ªå¯èƒ½çš„ç»“æœå­—æ®µ
        for key in ["content", "tool_output", "output", "response", "result"]:
            if key in item:
                func_response = item[key]
                break
        
        # å¦‚æœ content æ˜¯åˆ—è¡¨ï¼Œå°è¯•æå–æ–‡æœ¬
        if isinstance(func_response, list) and func_response:
            text_parts = [p.get("text", "") for p in func_response if isinstance(p, dict) and p.get("type") == "text"]
            if text_parts:
                func_response = "".join(text_parts)
        
        # ç¡®ä¿æœ‰å“åº”å†…å®¹
        if func_response is None:
            func_response = ""

        # Gemini è¦æ±‚ response ä¸º JSON å¯¹è±¡ï¼Œè‹¥ä¸ºåŸå§‹å­—ç¬¦ä¸²åˆ™åŒ…è£…
        if not isinstance(func_response, (dict, list)):
            func_response = {"content": str(func_response)}

        return {
            "functionResponse": {
                "name": func_name,
                "response": func_response
            }
        }
    
    def _map_finish_reason(self, reason: str, source_format: str, target_format: str) -> str:
        """æ˜ å°„ç»“æŸåŸå› """
        reason_mappings = {
            "openai": {
                "anthropic": {
                    "stop": "end_turn",
                    "length": "max_tokens",
                    "content_filter": "stop_sequence",
                    "tool_calls": "tool_use"
                }
            },
            "gemini": {
                "anthropic": {
                    # æ—§ç‰ˆæœ¬å¤§å†™æ ¼å¼
                    "STOP": "end_turn",
                    "MAX_TOKENS": "max_tokens",
                    "SAFETY": "stop_sequence",
                    "RECITATION": "stop_sequence",
                    # æ–°ç‰ˆæœ¬å°å†™æ ¼å¼ï¼ˆv1beta/v1 APIï¼‰
                    "stop": "end_turn",
                    "length": "max_tokens",
                    "safety": "stop_sequence",
                    "recitation": "stop_sequence",
                    "other": "end_turn"
                }
            }
        }
        
        try:
            return reason_mappings[source_format][target_format].get(reason, "end_turn")
        except KeyError:
            return "end_turn"

    def _sanitize_schema(self, schema: Dict[str, Any]) -> Dict[str, Any]:
        """é€’å½’ç§»é™¤Geminiä¸æ”¯æŒçš„JSON Schemaå…³é”®å­—"""
        if not isinstance(schema, dict):
            return schema

        allowed_keys = {"type", "description", "properties", "required", "enum", "items"}
        sanitized = {k: v for k, v in schema.items() if k in allowed_keys}

        if "properties" in sanitized and isinstance(sanitized["properties"], dict):
            sanitized["properties"] = {
                prop_name: self._sanitize_schema(prop_schema)
                for prop_name, prop_schema in sanitized["properties"].items()
            }

        if "items" in sanitized:
            sanitized["items"] = self._sanitize_schema(sanitized["items"])

        return sanitized

    def _clean_json_schema_properties(self, schema: Dict[str, Any]) -> Dict[str, Any]:
        """é€’å½’æ¸…ç†Geminiä¸æ”¯æŒçš„JSON Schemaå±æ€§"""
        if not isinstance(schema, dict):
            return schema

        # ç§»é™¤æ‰€æœ‰éæ ‡å‡†å±æ€§
        sanitized = {k: v for k, v in schema.items() if k in {"type", "description", "properties", "required", "enum", "items"}}

        if "properties" in sanitized and isinstance(sanitized["properties"], dict):
            sanitized["properties"] = {
                prop_name: self._clean_json_schema_properties(prop_schema)
                for prop_name, prop_schema in sanitized["properties"].items()
            }

        if "items" in sanitized:
            sanitized["items"] = self._clean_json_schema_properties(sanitized["items"])

        return sanitized

    def _deep_clean_for_gemini(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """æ·±åº¦æ¸…ç†Geminiè¯·æ±‚æ•°æ®ï¼Œç§»é™¤å¯èƒ½å¼•èµ·åè®®é”™è¯¯çš„å­—æ®µå’Œæ ¼å¼é—®é¢˜"""
        if not isinstance(data, dict):
            return data
        
        cleaned = {}
        
        for key, value in data.items():
            # å¤„ç† system_instruction
            if key == "system_instruction" and isinstance(value, dict):
                cleaned_si = {}
                if "parts" in value and isinstance(value["parts"], list):
                    clean_parts = []
                    for part in value["parts"]:
                        if isinstance(part, dict) and "text" in part:
                            # ç¡®ä¿textå­—æ®µæ˜¯çº¯å­—ç¬¦ä¸²ï¼Œæ— ç‰¹æ®Šå­—ç¬¦æˆ–ç¼–ç é—®é¢˜
                            text_content = str(part["text"]).strip()
                            if text_content:  # åªæ·»åŠ éç©ºæ–‡æœ¬
                                clean_parts.append({"text": text_content})
                    if clean_parts:
                        cleaned_si["parts"] = clean_parts
                        cleaned[key] = cleaned_si
            
            # å¤„ç† contents
            elif key == "contents" and isinstance(value, list):
                clean_contents = []
                for content in value:
                    if isinstance(content, dict):
                        clean_content = {}
                        # ç¡®ä¿roleå­—æ®µæ­£ç¡®
                        if "role" in content:
                            clean_content["role"] = str(content["role"])
                        # æ¸…ç†parts
                        if "parts" in content and isinstance(content["parts"], list):
                            clean_parts = []
                            for part in content["parts"]:
                                if isinstance(part, dict):
                                    clean_part = {}
                                    # åªä¿ç•™æ”¯æŒçš„å­—æ®µ
                                    if "text" in part:
                                        text_val = str(part["text"]).strip() if part["text"] else ""
                                        if text_val:
                                            clean_part["text"] = text_val
                                    elif "functionCall" in part:
                                        clean_part["functionCall"] = part["functionCall"]
                                    elif "functionResponse" in part:
                                        clean_part["functionResponse"] = part["functionResponse"]
                                    elif "inlineData" in part:
                                        clean_part["inlineData"] = part["inlineData"]
                                    
                                    if clean_part:  # åªæ·»åŠ éç©ºpart
                                        clean_parts.append(clean_part)
                            
                            if clean_parts:
                                clean_content["parts"] = clean_parts
                                clean_contents.append(clean_content)
                
                if clean_contents:
                    cleaned[key] = clean_contents
            
            # å¤„ç† generationConfig
            elif key == "generationConfig" and isinstance(value, dict):
                clean_gen_config = {}
                # åªä¿ç•™Geminiæ”¯æŒçš„ç”Ÿæˆé…ç½®å­—æ®µ
                allowed_gen_keys = {"temperature", "topP", "topK", "maxOutputTokens", "stopSequences", "thinkingConfig"}
                for gen_key, gen_value in value.items():
                    if gen_key in allowed_gen_keys and gen_value is not None:
                        clean_gen_config[gen_key] = gen_value
                cleaned[key] = clean_gen_config
            
            # å¤„ç† tools
            elif key == "tools" and isinstance(value, list):
                clean_tools = []
                for tool in value:
                    if isinstance(tool, dict) and "functionDeclarations" in tool:
                        clean_func_decls = []
                        for func_decl in tool["functionDeclarations"]:
                            if isinstance(func_decl, dict):
                                clean_decl = {}
                                if "name" in func_decl:
                                    clean_decl["name"] = str(func_decl["name"])
                                if "description" in func_decl:
                                    clean_decl["description"] = str(func_decl["description"])
                                if "parameters" in func_decl:
                                    # åº”ç”¨ç°æœ‰çš„schemaæ¸…ç†
                                    clean_decl["parameters"] = self._clean_json_schema_properties(func_decl["parameters"])
                                clean_func_decls.append(clean_decl)
                        
                        if clean_func_decls:
                            clean_tools.append({"functionDeclarations": clean_func_decls})
                
                if clean_tools:
                    cleaned[key] = clean_tools
            
            # å…¶ä»–å­—æ®µç›´æ¥ä¿ç•™
            else:
                cleaned[key] = value
        
        return cleaned