"""
Geminiæ ¼å¼è½¬æ¢å™¨
å¤„ç†Google Gemini APIæ ¼å¼ä¸å…¶ä»–æ ¼å¼ä¹‹é—´çš„è½¬æ¢
"""
from typing import Dict, Any, Optional, List
import json
import copy

from .base_converter import BaseConverter, ConversionResult, ConversionError


class GeminiConverter(BaseConverter):
    """Geminiæ ¼å¼è½¬æ¢å™¨"""
    
    def __init__(self):
        super().__init__()
        self.original_model = None
    
    def set_original_model(self, model: str):
        """è®¾ç½®åŸå§‹æ¨¡å‹åç§°"""
        self.original_model = model
    
    def _determine_reasoning_effort_from_budget(self, thinking_budget: Optional[int]) -> str:
        """æ ¹æ®thinkingBudgetåˆ¤æ–­OpenAI reasoning_effortç­‰çº§
        
        Args:
            thinking_budget: Gemini thinkingçš„thinkingBudgetå€¼
            
        Returns:
            str: OpenAI reasoning_effortç­‰çº§ ("low", "medium", "high")
        """
        import os
        
        # å¦‚æœæ²¡æœ‰æä¾›thinking_budgetæˆ–ä¸º-1ï¼ˆåŠ¨æ€æ€è€ƒï¼‰ï¼Œé»˜è®¤ä¸ºhigh
        if thinking_budget is None or thinking_budget == -1:
            reason = "dynamic thinking (-1)" if thinking_budget == -1 else "no budget provided"
            self.logger.info(f"No valid thinkingBudget ({reason}), defaulting to reasoning_effort='high'")
            return "high"
        
        # ä»ç¯å¢ƒå˜é‡è·å–é˜ˆå€¼é…ç½®
        low_threshold_str = os.environ.get("GEMINI_TO_OPENAI_LOW_REASONING_THRESHOLD")
        high_threshold_str = os.environ.get("GEMINI_TO_OPENAI_HIGH_REASONING_THRESHOLD")
        
        # æ£€æŸ¥å¿…éœ€çš„ç¯å¢ƒå˜é‡
        if low_threshold_str is None:
            raise ConversionError("GEMINI_TO_OPENAI_LOW_REASONING_THRESHOLD environment variable is required for intelligent reasoning_effort determination")
        
        if high_threshold_str is None:
            raise ConversionError("GEMINI_TO_OPENAI_HIGH_REASONING_THRESHOLD environment variable is required for intelligent reasoning_effort determination")
        
        try:
            low_threshold = int(low_threshold_str)
            high_threshold = int(high_threshold_str)
            
            self.logger.debug(f"Threshold configuration: low <= {low_threshold}, medium <= {high_threshold}, high > {high_threshold}")
            
            if thinking_budget <= low_threshold:
                effort = "low"
            elif thinking_budget <= high_threshold:
                effort = "medium"
            else:
                effort = "high"
            
            self.logger.info(f"ğŸ¯ Thinking budget {thinking_budget} -> reasoning_effort '{effort}' (thresholds: low<={low_threshold}, high<={high_threshold})")
            return effort
            
        except ValueError as e:
            raise ConversionError(f"Invalid threshold values in environment variables: {e}. GEMINI_TO_OPENAI_LOW_REASONING_THRESHOLD and GEMINI_TO_OPENAI_HIGH_REASONING_THRESHOLD must be integers.")
    
    def reset_streaming_state(self):
        """é‡ç½®æ‰€æœ‰æµå¼ç›¸å…³çš„çŠ¶æ€å˜é‡ï¼Œé¿å…çŠ¶æ€æ±¡æŸ“"""
        streaming_attrs = [
            '_anthropic_stream_id', '_openai_sent_start', '_gemini_text_started',
            '_anthropic_to_gemini_state'
        ]
        for attr in streaming_attrs:
            if hasattr(self, attr):
                delattr(self, attr)
    
    def get_supported_formats(self) -> List[str]:
        """è·å–æ”¯æŒçš„æ ¼å¼åˆ—è¡¨"""
        return ["openai", "anthropic", "gemini"]
    
    def convert_request(
        self,
        data: Dict[str, Any],
        target_format: str,
        headers: Optional[Dict[str, str]] = None
    ) -> ConversionResult:
        """è½¬æ¢Geminiè¯·æ±‚åˆ°ç›®æ ‡æ ¼å¼"""
        try:
            if target_format == "gemini":
                # Geminiåˆ°Geminiï¼Œæ ¼å¼ä¸æ¸ é“ç›¸åŒï¼Œä¸éœ€è¦è½¬æ¢æ€è€ƒå‚æ•°
                return ConversionResult(success=True, data=data)
            elif target_format == "openai":
                return self._convert_to_openai_request(data)
            elif target_format == "anthropic":
                return self._convert_to_anthropic_request(data)
            else:
                return ConversionResult(
                    success=False,
                    error=f"Unsupported target format: {target_format}"
                )
        except Exception as e:
            self.logger.error(f"Failed to convert Gemini request to {target_format}: {e}")
            return ConversionResult(success=False, error=str(e))
    
    def convert_response(
        self,
        data: Dict[str, Any],
        source_format: str,
        target_format: str
    ) -> ConversionResult:
        """è½¬æ¢å“åº”åˆ°Geminiæ ¼å¼"""
        try:
            if source_format == "gemini":
                return ConversionResult(success=True, data=data)
            elif source_format == "openai":
                return self._convert_from_openai_response(data)
            elif source_format == "anthropic":
                return self._convert_from_anthropic_response(data)
            else:
                return ConversionResult(
                    success=False,
                    error=f"Unsupported source format: {source_format}"
                )
        except Exception as e:
            self.logger.error(f"Failed to convert {source_format} response to Gemini: {e}")
            return ConversionResult(success=False, error=str(e))
    
    def _convert_to_openai_request(self, data: Dict[str, Any]) -> ConversionResult:
        """è½¬æ¢Geminiè¯·æ±‚åˆ°OpenAIæ ¼å¼"""
        result_data = {}
        
        # å¿…é¡»æœ‰åŸå§‹æ¨¡å‹åç§°ï¼Œå¦åˆ™æŠ¥é”™
        if not self.original_model:
            raise ValueError("Original model name is required for request conversion")
        
        result_data["model"] = self.original_model  # ä½¿ç”¨åŸå§‹æ¨¡å‹åç§°
        
        # åˆå§‹åŒ–å‡½æ•°è°ƒç”¨IDæ˜ å°„è¡¨ï¼Œç”¨äºä¿æŒå·¥å…·è°ƒç”¨å’Œå·¥å…·ç»“æœçš„IDä¸€è‡´æ€§
        # å…ˆæ‰«ææ•´ä¸ªå¯¹è¯å†å²ï¼Œä¸ºæ¯ä¸ªfunctionCallå’ŒfunctionResponseå»ºç«‹æ˜ å°„å…³ç³»
        self._function_call_mapping = self._build_function_call_mapping(data.get("contents", []))
        
        # å¤„ç†æ¶ˆæ¯å’Œç³»ç»Ÿæ¶ˆæ¯
        messages = []
        
        # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯ - æ”¯æŒä¸¤ç§æ ¼å¼
        system_instruction_data = data.get("systemInstruction") or data.get("system_instruction")
        if system_instruction_data:
            system_parts = system_instruction_data.get("parts", [])
            system_text = ""
            for part in system_parts:
                if "text" in part:
                    system_text += part["text"]
            if system_text:
                messages.append(self._create_system_message(system_text))
        
        # è½¬æ¢å†…å®¹
        if "contents" in data:
            for content in data["contents"]:
                gemini_role = content.get("role", "user")
                parts = content.get("parts", [])
                
                # å¤„ç†ä¸åŒè§’è‰²çš„æ¶ˆæ¯
                if gemini_role == "user":
                    # æ£€æŸ¥æ˜¯å¦åŒ…å« functionResponseï¼ˆå·¥å…·ç»“æœï¼‰
                    has_function_response = any("functionResponse" in part for part in parts)
                    if has_function_response:
                        # è½¬æ¢ä¸º OpenAI çš„ tool æ¶ˆæ¯
                        for part in parts:
                            if "functionResponse" in part:
                                fr = part["functionResponse"]
                                func_name = fr.get("name", "")
                                response_content = fr.get("response", {})
                                
                                # ä»å“åº”å†…å®¹ä¸­æå–æ–‡æœ¬
                                if isinstance(response_content, dict):
                                    tool_result = response_content.get("content", json.dumps(response_content, ensure_ascii=False))
                                else:
                                    tool_result = str(response_content)
                                
                                # ä½¿ç”¨é¢„å…ˆå»ºç«‹çš„æ˜ å°„è·å–å¯¹åº”çš„tool_call_id
                                if not hasattr(self, '_current_response_sequence'):
                                    self._current_response_sequence = {}
                                
                                sequence = self._current_response_sequence.get(func_name, 0) + 1
                                self._current_response_sequence[func_name] = sequence
                                
                                tool_call_id = self._function_call_mapping.get(f"response_{func_name}_{sequence}")
                                if not tool_call_id:
                                    # å¦‚æœæ²¡æœ‰æ˜ å°„ï¼Œç›´æ¥ä½¿ç”¨å¯¹åº”çš„call ID
                                    tool_call_id = self._function_call_mapping.get(f"{func_name}_{sequence}")
                                    if not tool_call_id:
                                        # æœ€åçš„å¤‡é€‰æ–¹æ¡ˆï¼šç”Ÿæˆæ–°çš„ID
                                        tool_call_id = f"call_{func_name}_{sequence:04d}"
                                
                                messages.append({
                                    "role": "tool",
                                    "tool_call_id": tool_call_id,
                                    "content": tool_result
                                })
                    else:
                        # æ™®é€šç”¨æˆ·æ¶ˆæ¯
                        message_content = self._convert_content_from_gemini(parts)
                        messages.append({
                            "role": "user",
                            "content": message_content
                        })
                        
                elif gemini_role == "model":
                    # åŠ©æ‰‹æ¶ˆæ¯ï¼Œå¯èƒ½åŒ…å«å·¥å…·è°ƒç”¨
                    message_content = self._convert_content_from_gemini(parts)
                    
                    if isinstance(message_content, dict) and message_content.get("type") == "tool_calls":
                        # æœ‰å·¥å…·è°ƒç”¨çš„åŠ©æ‰‹æ¶ˆæ¯
                        message = {
                            "role": "assistant",
                            "content": message_content.get("content"),
                            "tool_calls": message_content["tool_calls"]
                        }
                        messages.append(message)
                    else:
                        # æ™®é€šåŠ©æ‰‹æ¶ˆæ¯
                        messages.append({
                            "role": "assistant",
                            "content": message_content
                        })
                        
                elif gemini_role == "tool":
                    # å·¥å…·è§’è‰²çš„æ¶ˆæ¯ï¼Œå¤„ç†functionResponse
                    for part in parts:
                        if "functionResponse" in part:
                            fr = part["functionResponse"]
                            func_name = fr.get("name", "")
                            response_content = fr.get("response", {})
                            
                            # ä»å“åº”å†…å®¹ä¸­æå–æ–‡æœ¬
                            if isinstance(response_content, dict):
                                tool_result = response_content.get("content", json.dumps(response_content, ensure_ascii=False))
                            else:
                                tool_result = str(response_content)
                            
                            # ä½¿ç”¨é¢„å…ˆå»ºç«‹çš„æ˜ å°„è·å–å¯¹åº”çš„tool_call_id
                            if not hasattr(self, '_current_response_sequence'):
                                self._current_response_sequence = {}
                            
                            sequence = self._current_response_sequence.get(func_name, 0) + 1
                            self._current_response_sequence[func_name] = sequence
                            
                            tool_call_id = self._function_call_mapping.get(f"response_{func_name}_{sequence}")
                            if not tool_call_id:
                                # å¦‚æœæ²¡æœ‰æ˜ å°„ï¼Œç›´æ¥ä½¿ç”¨å¯¹åº”çš„call ID
                                tool_call_id = self._function_call_mapping.get(f"{func_name}_{sequence}")
                                if not tool_call_id:
                                    # æœ€åçš„å¤‡é€‰æ–¹æ¡ˆï¼šç”Ÿæˆæ–°çš„ID
                                    tool_call_id = f"call_{func_name}_{sequence:04d}"
                            
                            messages.append({
                                "role": "tool",
                                "tool_call_id": tool_call_id,
                                "content": tool_result
                            })
                else:
                    # å…¶ä»–è§’è‰²ï¼Œé»˜è®¤è½¬ä¸ºassistant
                    message_content = self._convert_content_from_gemini(parts)
                    messages.append({
                        "role": "assistant",
                        "content": message_content
                    })
        
        result_data["messages"] = messages
        
        # å¤„ç†ç”Ÿæˆé…ç½®
        if "generationConfig" in data:
            config = data["generationConfig"]
            if "temperature" in config:
                result_data["temperature"] = config["temperature"]
            if "topP" in config:
                result_data["top_p"] = config["topP"]
            if "maxOutputTokens" in config:
                result_data["max_tokens"] = config["maxOutputTokens"]
            if "stopSequences" in config:
                result_data["stop"] = config["stopSequences"]
            
            # å¤„ç†ç»“æ„åŒ–è¾“å‡º
            if config.get("response_mime_type") == "application/json":
                result_data["response_format"] = {"type": "json_object"}
                if "response_schema" in config:
                    result_data["response_format"] = {
                        "type": "json_schema",
                        "json_schema": {
                            "name": "response",
                            "strict": True,
                            "schema": config["response_schema"]
                        }
                    }
        
        # å¤„ç†å·¥å…·è°ƒç”¨
        if "tools" in data:
            openai_tools = []
            for tool in data["tools"]:
                # Geminiå®˜æ–¹ä½¿ç”¨ snake_case: function_declarations
                func_key = None
                if "function_declarations" in tool:
                    func_key = "function_declarations"
                elif "functionDeclarations" in tool:  # å…¼å®¹æ—§å†™æ³•
                    func_key = "functionDeclarations"
                if func_key:
                    for func_decl in tool[func_key]:
                        openai_tools.append({
                            "type": "function",
                            "function": {
                                "name": func_decl.get("name", ""),
                                "description": func_decl.get("description", ""),
                                "parameters": self._sanitize_schema_for_openai(func_decl.get("parameters", {}))
                            }
                        })
            if openai_tools:
                result_data["tools"] = openai_tools
                result_data["tool_choice"] = "auto"
        
        # å¤„ç†æ€è€ƒé¢„ç®—è½¬æ¢ (Gemini thinkingConfig -> OpenAI reasoning_effort + max_completion_tokens)
        if "generationConfig" in data and "thinkingConfig" in data["generationConfig"]:
            thinking_config = data["generationConfig"]["thinkingConfig"]
            thinking_budget = thinking_config.get("thinkingBudget")
            
            if thinking_budget is not None and thinking_budget != 0:
                # æ£€æµ‹åˆ°æ€è€ƒå‚æ•°ï¼Œè®¾ç½®ä¸ºOpenAIæ€è€ƒæ¨¡å‹æ ¼å¼
                reasoning_effort = self._determine_reasoning_effort_from_budget(thinking_budget)
                result_data["reasoning_effort"] = reasoning_effort
                
                # å¤„ç†max_completion_tokensçš„ä¼˜å…ˆçº§é€»è¾‘
                max_completion_tokens = None
                
                # ä¼˜å…ˆçº§1ï¼šå®¢æˆ·ç«¯ä¼ å…¥çš„maxOutputTokens
                if "generationConfig" in data and "maxOutputTokens" in data["generationConfig"]:
                    max_completion_tokens = data["generationConfig"]["maxOutputTokens"]
                    # ç§»é™¤max_tokensï¼Œä½¿ç”¨max_completion_tokens
                    if "max_tokens" in result_data:
                        result_data.pop("max_tokens", None)
                    self.logger.info(f"Using client maxOutputTokens as max_completion_tokens: {max_completion_tokens}")
                else:
                    # ä¼˜å…ˆçº§2ï¼šç¯å¢ƒå˜é‡OPENAI_REASONING_MAX_TOKENS
                    import os
                    env_max_tokens = os.environ.get("OPENAI_REASONING_MAX_TOKENS")
                    if env_max_tokens:
                        try:
                            max_completion_tokens = int(env_max_tokens)
                            self.logger.info(f"Using OPENAI_REASONING_MAX_TOKENS from environment: {max_completion_tokens}")
                        except ValueError:
                            self.logger.warning(f"Invalid OPENAI_REASONING_MAX_TOKENS value '{env_max_tokens}', must be integer")
                            env_max_tokens = None
                    
                    if not env_max_tokens:
                        # ä¼˜å…ˆçº§3ï¼šéƒ½æ²¡æœ‰åˆ™æŠ¥é”™
                        raise ConversionError("For OpenAI reasoning models, max_completion_tokens is required. Please specify maxOutputTokens in generationConfig or set OPENAI_REASONING_MAX_TOKENS environment variable.")
                
                result_data["max_completion_tokens"] = max_completion_tokens
                self.logger.info(f"Gemini thinkingBudget {thinking_budget} -> OpenAI reasoning_effort='{reasoning_effort}', max_completion_tokens={max_completion_tokens}")
        
        # å¤„ç†æµå¼å‚æ•° - å…³é”®ä¿®å¤ï¼
        if "stream" in data:
            result_data["stream"] = data["stream"]
        
        return ConversionResult(success=True, data=result_data)
    
    def _convert_to_anthropic_request(self, data: Dict[str, Any]) -> ConversionResult:
        """è½¬æ¢Geminiè¯·æ±‚åˆ°Anthropicæ ¼å¼"""
        result_data = {}
        
        # å¿…é¡»æœ‰åŸå§‹æ¨¡å‹åç§°ï¼Œå¦åˆ™æŠ¥é”™
        if not self.original_model:
            raise ValueError("Original model name is required for request conversion")
        
        result_data["model"] = self.original_model  # ä½¿ç”¨åŸå§‹æ¨¡å‹åç§°
        
        # å¤„ç†ç³»ç»Ÿæ¶ˆæ¯ - æ”¯æŒä¸¤ç§æ ¼å¼
        system_instruction_data = data.get("systemInstruction") or data.get("system_instruction")
        if system_instruction_data:
            system_parts = system_instruction_data.get("parts", [])
            system_text = ""
            for part in system_parts:
                if "text" in part:
                    system_text += part["text"]
            if system_text:
                result_data["system"] = system_text
        
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        if "contents" in data:
            # å»ºç«‹å·¥å…·è°ƒç”¨IDæ˜ å°„è¡¨
            self._function_call_mapping = self._build_function_call_mapping(data["contents"])
            
            anthropic_messages = []
            for content in data["contents"]:
                role = content.get("role", "user")
                if role == "model":
                    role = "assistant"
                elif role == "tool":
                    # Geminiçš„toolè§’è‰²ï¼ˆfunctionResponseï¼‰å¯¹åº”Anthropicçš„userè§’è‰²
                    role = "user"
                
                message_content = self._convert_content_to_anthropic(content.get("parts", []))
                
                # è·³è¿‡ç©ºå†…å®¹çš„æ¶ˆæ¯ï¼ŒAnthropicä¸å…è®¸ç©ºå†…å®¹
                if not message_content or (isinstance(message_content, str) and not message_content.strip()):
                    self.logger.warning(f"Skipping message with empty content for role '{role}'")
                    continue
                
                anthropic_messages.append({
                    "role": role,
                    "content": message_content
                })
            result_data["messages"] = anthropic_messages
        
        # å¤„ç†ç”Ÿæˆé…ç½®
        if "generationConfig" in data:
            config = data["generationConfig"]
            if "temperature" in config:
                result_data["temperature"] = config["temperature"]
            if "topP" in config:
                result_data["top_p"] = config["topP"]
            if "topK" in config:
                result_data["top_k"] = config["topK"]
            if "maxOutputTokens" in config:
                result_data["max_tokens"] = config["maxOutputTokens"]
            if "stopSequences" in config:
                result_data["stop_sequences"] = config["stopSequences"]
        
        # Anthropic è¦æ±‚å¿…é¡»æœ‰ max_tokensï¼ŒæŒ‰ä¼˜å…ˆçº§å¤„ç†ï¼š
        # 1. Gemini generationConfigä¸­çš„maxOutputTokensï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
        # 2. ç¯å¢ƒå˜é‡ANTHROPIC_MAX_TOKENS
        # 3. éƒ½æ²¡æœ‰åˆ™æŠ¥é”™
        if "max_tokens" not in result_data:
            # ä¼˜å…ˆçº§2ï¼šæ£€æŸ¥ç¯å¢ƒå˜é‡ANTHROPIC_MAX_TOKENS
            import os
            env_max_tokens = os.environ.get("ANTHROPIC_MAX_TOKENS")
            if env_max_tokens:
                try:
                    max_tokens = int(env_max_tokens)
                    self.logger.info(f"Using ANTHROPIC_MAX_TOKENS from environment: {max_tokens}")
                    result_data["max_tokens"] = max_tokens
                except ValueError:
                    self.logger.warning(f"Invalid ANTHROPIC_MAX_TOKENS value '{env_max_tokens}', must be integer")
                    env_max_tokens = None
            
            if not env_max_tokens:
                # ä¼˜å…ˆçº§3ï¼šéƒ½æ²¡æœ‰åˆ™æŠ¥é”™ï¼Œè¦æ±‚ç”¨æˆ·æ˜ç¡®æŒ‡å®š
                raise ValueError(f"max_tokens is required for Anthropic API. Please specify max_tokens in generationConfig.maxOutputTokens or set ANTHROPIC_MAX_TOKENS environment variable.")
        
        # å¤„ç†å·¥å…·è°ƒç”¨
        if "tools" in data:
            anthropic_tools = []
            for tool in data["tools"]:
                func_key = None
                if "function_declarations" in tool:
                    func_key = "function_declarations"
                elif "functionDeclarations" in tool:  # å…¼å®¹æ—§å†™æ³•
                    func_key = "functionDeclarations"
                if func_key:
                    for func_decl in tool[func_key]:
                        anthropic_tools.append({
                            "name": func_decl.get("name", ""),
                            "description": func_decl.get("description", ""),
                            "input_schema": self._convert_schema_for_anthropic(func_decl.get("parameters", {}))
                        })
            if anthropic_tools:
                result_data["tools"] = anthropic_tools
        
        # å¤„ç†æ€è€ƒé¢„ç®—è½¬æ¢ (Gemini thinkingBudget -> Anthropic thinkingBudget)
        if "generationConfig" in data and "thinkingConfig" in data["generationConfig"]:
            thinking_config = data["generationConfig"]["thinkingConfig"]
            thinking_budget = thinking_config.get("thinkingBudget")
            
            if thinking_budget is not None:
                if thinking_budget == -1:
                    # åŠ¨æ€æ€è€ƒï¼Œå¯ç”¨ä½†ä¸è®¾ç½®å…·ä½“tokenæ•°
                    result_data["thinking"] = {
                        "type": "enabled"
                    }
                    self.logger.info("Gemini thinkingBudget -1 (dynamic) -> Anthropic thinking enabled without budget")
                elif thinking_budget == 0:
                    # ä¸å¯ç”¨æ€è€ƒ
                    pass
                else:
                    # æ•°å€¼å‹æ€è€ƒé¢„ç®—ï¼Œç›´æ¥è½¬æ¢
                    result_data["thinking"] = {
                        "type": "enabled",
                        "budget_tokens": thinking_budget
                    }
                    self.logger.info(f"Gemini thinkingBudget {thinking_budget} -> Anthropic thinkingBudget {thinking_budget}")
        
        # å¤„ç†æµå¼å‚æ•° - å…³é”®ä¿®å¤ï¼
        if "stream" in data:
            result_data["stream"] = data["stream"]
        
        return ConversionResult(success=True, data=result_data)
    
    def _convert_from_openai_response(self, data: Dict[str, Any]) -> ConversionResult:
        """è½¬æ¢OpenAIå“åº”åˆ°Geminiæ ¼å¼"""
        result_data = {
            "candidates": [],
            "usageMetadata": {}
        }
        
        # å¤„ç†é€‰æ‹©
        if "choices" in data and data["choices"] and data["choices"][0]:
            choice = data["choices"][0]
            message = choice.get("message", {})
            content = message.get("content", "")
            tool_calls = message.get("tool_calls", [])
            
            # æ„å»º parts
            parts = []
            
            # æ·»åŠ æ–‡æœ¬å†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
            if content:
                parts.append({"text": content})
            
            # æ·»åŠ å·¥å…·è°ƒç”¨ï¼ˆå¦‚æœæœ‰ï¼‰
            if tool_calls:
                for tool_call in tool_calls:
                    if tool_call and tool_call.get("type") == "function" and "function" in tool_call:
                        func = tool_call["function"]
                        func_name = func.get("name", "")
                        # OpenAI çš„ arguments æ˜¯ JSON å­—ç¬¦ä¸²ï¼Œéœ€è¦è§£æ
                        args_str = func.get("arguments", "{}")
                        try:
                            func_args = json.loads(args_str) if args_str else {}
                        except json.JSONDecodeError:
                            func_args = {}
                        
                        parts.append({
                            "functionCall": {
                                "name": func_name,
                                "args": func_args
                            }
                        })
            
            # å¦‚æœæ²¡æœ‰ä»»ä½•å†…å®¹ï¼Œæ·»åŠ ç©ºæ–‡æœ¬
            if not parts:
                parts = [{"text": ""}]
            
            candidate = {
                "content": {
                    "parts": parts,
                    "role": "model"
                },
                "finishReason": self._map_finish_reason(choice.get("finish_reason", "stop"), "openai", "gemini"),
                "index": 0
            }
            result_data["candidates"] = [candidate]
        
        # å¤„ç†ä½¿ç”¨æƒ…å†µ
        if "usage" in data and data["usage"] is not None:
            usage = data["usage"]
            result_data["usageMetadata"] = {
                "promptTokenCount": usage.get("prompt_tokens", 0),
                "candidatesTokenCount": usage.get("completion_tokens", 0),
                "totalTokenCount": usage.get("total_tokens", 0)
            }
        
        return ConversionResult(success=True, data=result_data)
    
    def _convert_from_anthropic_response(self, data: Dict[str, Any]) -> ConversionResult:
        """è½¬æ¢Anthropicå“åº”åˆ°Geminiæ ¼å¼"""
        result_data = {
            "candidates": [],
            "usageMetadata": {}
        }
        
        # å¤„ç†å†…å®¹ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€å·¥å…·è°ƒç”¨å’Œæ€è€ƒå†…å®¹
        parts = []
        if "content" in data and isinstance(data["content"], list):
            for item in data["content"]:
                item_type = item.get("type")
                
                # å¤„ç†æ–‡æœ¬å†…å®¹
                if item_type == "text":
                    text_content = item.get("text", "")
                    if text_content.strip():  # åªæ·»åŠ éç©ºæ–‡æœ¬
                        parts.append({"text": text_content})
                
                # å¤„ç†æ€è€ƒå†…å®¹ (thinking â†’ text with thought: true)
                elif item_type == "thinking":
                    thinking_content = item.get("thinking", "")
                    if thinking_content.strip():
                        parts.append({
                            "text": thinking_content,
                            "thought": True  # Gemini 2025æ ¼å¼çš„thinkingæ ‡è¯†
                        })
                
                # å¤„ç†å·¥å…·è°ƒç”¨ (tool_use â†’ functionCall)
                elif item_type == "tool_use":
                    parts.append({
                        "functionCall": {
                            "name": item.get("name", ""),
                            "args": item.get("input", {})
                        }
                    })
        
        # å¦‚æœæ²¡æœ‰ä»»ä½•å†…å®¹ï¼Œæ·»åŠ ç©ºæ–‡æœ¬é¿å…ç©ºpartsæ•°ç»„
        if not parts:
            parts = [{"text": ""}]
        
        candidate = {
            "content": {
                "parts": parts,
                "role": "model"
            },
            "finishReason": self._map_finish_reason(data.get("stop_reason", "end_turn"), "anthropic", "gemini"),
            "index": 0
        }
        result_data["candidates"] = [candidate]
        
        # å¤„ç†ä½¿ç”¨æƒ…å†µ
        if "usage" in data and data["usage"] is not None:
            usage = data["usage"]
            input_tokens = usage.get("input_tokens", 0)
            output_tokens = usage.get("output_tokens", 0)
            result_data["usageMetadata"] = {
                "promptTokenCount": input_tokens,
                "candidatesTokenCount": output_tokens,
                "totalTokenCount": input_tokens + output_tokens
            }
        
        return ConversionResult(success=True, data=result_data)
    
    def _convert_from_openai_streaming_chunk(self, data: Dict[str, Any]) -> ConversionResult:
        """è½¬æ¢OpenAIæµå¼å“åº”chunkåˆ°Geminiæ ¼å¼"""
        self.logger.info(f"OPENAI->GEMINI CHUNK: {data}")  # è®°å½•è¾“å…¥æ•°æ®
        
        # ä¸ºæµå¼å·¥å…·è°ƒç”¨ç»´æŠ¤çŠ¶æ€
        if not hasattr(self, '_streaming_tool_calls'):
            self._streaming_tool_calls = {}
        # å…ˆå¤„ç†å¢é‡å†…å®¹å’Œå·¥å…·è°ƒç”¨ï¼ˆæ”¶é›†çŠ¶æ€ï¼‰
        if "choices" in data and data["choices"] and data["choices"][0]:
            choice = data["choices"][0]
            delta = choice.get("delta", {})
            
            # æ”¶é›†æµå¼å·¥å…·è°ƒç”¨ä¿¡æ¯
            if "tool_calls" in delta:
                for tool_call in delta["tool_calls"]:
                    call_index = tool_call.get("index", 0)
                    call_id = tool_call.get("id", "")
                    call_type = tool_call.get("type", "function")
                    
                    # åˆå§‹åŒ–å·¥å…·è°ƒç”¨çŠ¶æ€
                    if call_index not in self._streaming_tool_calls:
                        self._streaming_tool_calls[call_index] = {
                            "id": call_id,
                            "type": call_type,
                            "function": {
                                "name": "",
                                "arguments": ""
                            }
                        }
                    
                    # æ›´æ–°å·¥å…·è°ƒç”¨ä¿¡æ¯
                    if "function" in tool_call:
                        func = tool_call["function"]
                        if "name" in func:
                            self._streaming_tool_calls[call_index]["function"]["name"] = func["name"]
                        if "arguments" in func:
                            self._streaming_tool_calls[call_index]["function"]["arguments"] += func["arguments"]
                    
                    self.logger.debug(f"Updated tool call {call_index}: {self._streaming_tool_calls[call_index]}")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯å®Œæ•´çš„æµå¼å“åº”ç»“æŸ
        if "choices" in data and data["choices"] and data["choices"][0] and data["choices"][0].get("finish_reason"):
            choice = data["choices"][0]
            delta = choice.get("delta", {})
            content = delta.get("content", "")
            
            # æ„å»ºpartsæ•°ç»„ï¼Œå¯èƒ½åŒ…å«å†…å®¹å’Œå·¥å…·è°ƒç”¨
            parts = []
            
            # å¤„ç†æ–‡æœ¬å†…å®¹
            if content:
                parts.append({"text": content})
            
            # å¤„ç†æ”¶é›†åˆ°çš„å·¥å…·è°ƒç”¨
            if self._streaming_tool_calls:
                self.logger.debug(f"FINISH: Processing collected tool calls: {self._streaming_tool_calls}")
                for call_index, tool_call in self._streaming_tool_calls.items():
                    func = tool_call.get("function", {})
                    func_name = func.get("name", "")
                    func_args = func.get("arguments", "{}")
                    self.logger.debug(f"FINISH TOOL CALL - name: {func_name}, args: '{func_args}'")

                    # OpenAI çš„ arguments å­—æ®µæ˜¯ JSON å­—ç¬¦ä¸²ï¼Œéœ€è¦è§£æ
                    if func_args.strip() == "[DONE]":
                        self.logger.warning(f"Found [DONE] in tool call arguments, skipping")
                        continue
                    try:
                        func_args_json = json.loads(func_args) if isinstance(func_args, str) else func_args
                    except json.JSONDecodeError as e:
                        self.logger.error(f"JSON decode error in tool args: {e}, func_args: '{func_args}'")
                        func_args_json = {}

                    parts.append({
                        "functionCall": {
                            "name": func_name,
                            "args": func_args_json
                        }
                    })
                
                # æ¸…ç†å·¥å…·è°ƒç”¨çŠ¶æ€ï¼Œä¸ºä¸‹æ¬¡è¯·æ±‚åšå‡†å¤‡
                self._streaming_tool_calls = {}
            
            # è¿™æ˜¯æœ€åçš„chunkï¼ŒåŒ…å«finish_reason
            result_data = {
                "candidates": [{
                    "content": {
                        "parts": parts,
                        "role": "model"
                    },
                    "finishReason": self._map_finish_reason(choice.get("finish_reason", "stop"), "openai", "gemini"),
                    "index": 0
                }]
            }
            
            # æ·»åŠ usageä¿¡æ¯ï¼ˆå¦‚æœæœ‰ä¸”ä¸ä¸ºNoneï¼‰
            if "usage" in data and data["usage"] is not None:
                usage = data["usage"]
                result_data["usageMetadata"] = {
                    "promptTokenCount": usage.get("prompt_tokens", 0),
                    "candidatesTokenCount": usage.get("completion_tokens", 0),
                    "totalTokenCount": usage.get("total_tokens", 0)
                }
            
            return ConversionResult(success=True, data=result_data)
            
        # æ£€æŸ¥æ˜¯å¦æœ‰å¢é‡å†…å®¹ï¼ˆéfinish chunkï¼‰
        elif "choices" in data and data["choices"] and data["choices"][0]:
            choice = data["choices"][0]
            delta = choice.get("delta", {})
            content = delta.get("content", "")
            tool_calls = delta.get("tool_calls", [])
            
            parts = []
            
            # å¤„ç†æ–‡æœ¬å†…å®¹
            if content:
                parts.append({"text": content})
            
            # å¯¹äºå·¥å…·è°ƒç”¨chunksï¼Œæˆ‘ä»¬å·²ç»åœ¨ä¸Šé¢æ”¶é›†äº†ï¼Œè¿™é‡Œä¸éœ€è¦å†å¤„ç†
            # åªæœ‰å½“æœ‰æ–‡æœ¬å†…å®¹æ—¶æ‰å‘é€chunkç»™å®¢æˆ·ç«¯
            if tool_calls:
                self.logger.debug(f"Skipping tool call chunk (already collected): {tool_calls}")
            
            # åªæœ‰åœ¨æœ‰æ–‡æœ¬å†…å®¹æ—¶æ‰åˆ›å»ºchunk
            if parts:
                result_data = {
                    "candidates": [{
                        "content": {
                            "parts": parts,
                            "role": "model"
                        },
                        "index": 0
                    }]
                }
                
                return ConversionResult(success=True, data=result_data)
        
        # å¦‚æœæ²¡æœ‰å†…å®¹ä¹Ÿæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œåˆ™è¿”å›ç©º candidatesï¼Œä¿æŒ SSE è¿æ¥
        result_data = {
            "candidates": [{
                "content": {
                    "parts": [],
                    "role": "model"
                },
                "index": 0
            }]
        }
        
        return ConversionResult(success=True, data=result_data)
    
    def _convert_from_anthropic_streaming_chunk(self, data: Dict[str, Any]) -> ConversionResult:
        """è½¬æ¢Anthropicæµå¼å“åº”chunkåˆ°Geminiæ ¼å¼"""
        import json
        import random
        
        # åˆå§‹åŒ–æµçŠ¶æ€å˜é‡
        if not hasattr(self, '_anthropic_to_gemini_state'):
            self._anthropic_to_gemini_state = {
                'current_text': '',
                'current_tool_calls': {},  # index -> tool_call_info
                'has_started': False
            }
        
        state = self._anthropic_to_gemini_state
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯message_deltaç±»å‹çš„ç»“æŸ
        if data.get("type") == "message_delta" and "delta" in data and "stop_reason" in data["delta"]:
            # è¿™æ˜¯æœ€åçš„chunkï¼ŒåªåŒ…å«å·¥å…·è°ƒç”¨å’Œç»“æŸä¿¡æ¯ï¼Œä¸é‡å¤å‘é€æ–‡æœ¬
            parts = []
            
            # åªæ·»åŠ å·¥å…·è°ƒç”¨ï¼ˆæ–‡æœ¬å†…å®¹å·²ç»åœ¨ä¹‹å‰çš„text_deltaä¸­å‘é€è¿‡äº†ï¼‰
            for tool_info in state['current_tool_calls'].values():
                if tool_info.get('name') and tool_info.get('complete_args'):
                    try:
                        args_obj = json.loads(tool_info['complete_args'])
                        parts.append({
                            "functionCall": {
                                "name": tool_info['name'],
                                "args": args_obj
                            }
                        })
                    except json.JSONDecodeError:
                        # å¦‚æœJSONè§£æå¤±è´¥ï¼Œä½¿ç”¨ç©ºå‚æ•°
                        parts.append({
                            "functionCall": {
                                "name": tool_info['name'],
                                "args": {}
                            }
                        })
            
            # ç¡®å®šfinish reason
            stop_reason = data["delta"]["stop_reason"]
            if stop_reason == "tool_use":
                finish_reason = "STOP"  # Geminiä¸­å·¥å…·è°ƒç”¨ä¹Ÿç”¨STOP
            else:
                finish_reason = self._map_finish_reason(stop_reason, "anthropic", "gemini")
            
            result_data = {
                "candidates": [{
                    "content": {
                        "parts": parts,
                        "role": "model"
                    },
                    "finishReason": finish_reason,
                    "index": 0
                }]
            }
            
            # æ·»åŠ usageä¿¡æ¯ï¼ˆå¦‚æœæœ‰ä¸”ä¸ä¸ºNoneï¼‰
            if "usage" in data and data["usage"] is not None:
                usage = data["usage"]
                result_data["usageMetadata"] = {
                    "promptTokenCount": usage.get("input_tokens", 0),
                    "candidatesTokenCount": usage.get("output_tokens", 0),
                    "totalTokenCount": usage.get("input_tokens", 0) + usage.get("output_tokens", 0)
                }
            
            # æ¸…ç†çŠ¶æ€
            if hasattr(self, '_anthropic_to_gemini_state'):
                delattr(self, '_anthropic_to_gemini_state')
            
            return ConversionResult(success=True, data=result_data)
        
        # å¤„ç†content_block_start - å·¥å…·è°ƒç”¨å¼€å§‹
        elif data.get("type") == "content_block_start" and "content_block" in data:
            content_block = data["content_block"]
            index = data.get("index", 0)
            
            if content_block.get("type") == "tool_use":
                # è®°å½•å·¥å…·è°ƒç”¨ä¿¡æ¯
                state['current_tool_calls'][index] = {
                    'id': content_block.get("id", ""),
                    'name': content_block.get("name", ""),
                    'complete_args': ''
                }
            
            # å¯¹äºcontent_block_startï¼Œä¸è¿”å›ä»»ä½•å†…å®¹
            return ConversionResult(success=True, data={})
        
        # å¤„ç†content_block_delta - å¢é‡å†…å®¹
        elif data.get("type") == "content_block_delta" and "delta" in data:
            delta = data["delta"]
            index = data.get("index", 0)
            
            # æ–‡æœ¬å¢é‡
            if delta.get("type") == "text_delta":
                text_content = delta.get("text", "")
                if text_content:
                    state['current_text'] += text_content
                    
                    # å®æ—¶è¿”å›æ–‡æœ¬å¢é‡
                    result_data = {
                        "candidates": [{
                            "content": {
                                "parts": [{"text": text_content}],
                                "role": "model"
                            },
                            "index": 0
                        }]
                    }
                    return ConversionResult(success=True, data=result_data)
            
            # å·¥å…·è°ƒç”¨å‚æ•°å¢é‡
            elif delta.get("type") == "input_json_delta":
                partial_json = delta.get("partial_json", "")
                if index in state['current_tool_calls']:
                    state['current_tool_calls'][index]['complete_args'] += partial_json
                
                # å¯¹äºå·¥å…·å‚æ•°å¢é‡ï¼Œä¸è¿”å›å®æ—¶å†…å®¹
                return ConversionResult(success=True, data={})
        
        # å¤„ç†content_block_stop
        elif data.get("type") == "content_block_stop":
            # å¯¹äºcontent_block_stopï¼Œä¸è¿”å›ä»»ä½•å†…å®¹
            return ConversionResult(success=True, data={})
        
        # å¤„ç†message_start
        elif data.get("type") == "message_start":
            # å¼ºåˆ¶é‡ç½®çŠ¶æ€ï¼Œç¡®ä¿æ–°æµå¼€å§‹æ—¶çŠ¶æ€å¹²å‡€
            self._anthropic_to_gemini_state = {
                'current_text': '',
                'current_tool_calls': {},  # index -> tool_call_info
                'has_started': True
            }
            # å¯¹äºmessage_startï¼Œä¸è¿”å›ä»»ä½•å†…å®¹
            return ConversionResult(success=True, data={})
        
        # å¤„ç†message_stop
        elif data.get("type") == "message_stop":
            # æ¸…ç†çŠ¶æ€
            if hasattr(self, '_anthropic_to_gemini_state'):
                delattr(self, '_anthropic_to_gemini_state')
            return ConversionResult(success=True, data={})
        
        # å…¶ä»–ç±»å‹çš„äº‹ä»¶ï¼Œè¿”å›ç©ºç»“æ„
        return ConversionResult(success=True, data={})
    
    def _convert_from_gemini_streaming_chunk(self, data: Dict[str, Any]) -> ConversionResult:
        """è½¬æ¢Geminiæµå¼å“åº”chunkåˆ°ç›®æ ‡æ ¼å¼"""
        # Geminiæµå¼å“åº”é€šå¸¸åŒ…å«candidatesæ•°ç»„
        if "candidates" in data and data["candidates"]:
            candidate = data["candidates"][0]
            content = candidate.get("content", {})
            parts = content.get("parts", [])
            finish_reason = candidate.get("finishReason")
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨
            has_function_call = any("functionCall" in part for part in parts)
            has_text = any("text" in part for part in parts)
            
            result_data = {
                "candidates": [{
                    "content": {
                        "parts": parts,
                        "role": "model"
                    },
                    "index": 0
                }]
            }
            
            # æ·»åŠ finishReasonï¼ˆå¦‚æœæœ‰ï¼‰
            if finish_reason:
                result_data["candidates"][0]["finishReason"] = finish_reason
            
            # å¤„ç†usageä¿¡æ¯
            if "usageMetadata" in data:
                result_data["usageMetadata"] = data["usageMetadata"]
            
            return ConversionResult(success=True, data=result_data)
        
        # å¦‚æœæ²¡æœ‰candidatesï¼Œè¿”å›ç©ºçš„ç»“æ„
        result_data = {
            "candidates": [{
                "content": {
                    "parts": [],
                    "role": "model"
                },
                "index": 0
            }]
        }
        
        return ConversionResult(success=True, data=result_data)
    
    def _convert_content_from_gemini(self, parts: List[Dict[str, Any]]) -> Any:
        """è½¬æ¢Geminiå†…å®¹åˆ°é€šç”¨æ ¼å¼"""
        if len(parts) == 1 and "text" in parts[0]:
            return parts[0]["text"]
        
        # å¤„ç†å¤šæ¨¡æ€å†…å®¹å’Œå·¥å…·è°ƒç”¨
        converted_content = []
        has_tool_calls = False
        tool_calls = []
        text_content = ""
        
        for part in parts:
            if "text" in part:
                text_content += part["text"]
                converted_content.append({
                    "type": "text",
                    "text": part["text"]
                })
            elif "inlineData" in part:
                # è½¬æ¢å›¾åƒæ ¼å¼
                inline_data = part["inlineData"]
                mime_type = inline_data.get("mimeType", "image/jpeg")
                data_part = inline_data.get("data", "")
                converted_content.append({
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:{mime_type};base64,{data_part}"
                    }
                })
            elif "functionCall" in part:
                # è½¬æ¢å‡½æ•°è°ƒç”¨
                fc = part["functionCall"]
                func_name = fc.get("name", "")
                func_args = fc.get("args", {})
                
                # ä½¿ç”¨é¢„å…ˆå»ºç«‹çš„IDæ˜ å°„
                # ä¸ºæ¯ä¸ªå‡½æ•°è°ƒç”¨ä½¿ç”¨åºåˆ—å·ç¡®ä¿å”¯ä¸€æ€§
                if not hasattr(self, '_current_call_sequence'):
                    self._current_call_sequence = {}
                
                sequence = self._current_call_sequence.get(func_name, 0) + 1
                self._current_call_sequence[func_name] = sequence
                
                tool_call_id = self._function_call_mapping.get(f"{func_name}_{sequence}")
                if not tool_call_id:
                    # å¦‚æœæ˜ å°„ä¸­æ²¡æœ‰æ‰¾åˆ°ï¼Œç”Ÿæˆæ–°çš„ID
                    tool_call_id = f"call_{func_name}_{sequence:04d}"
                
                tool_calls.append({
                    "id": tool_call_id,
                    "type": "function",
                    "function": {
                        "name": func_name,
                        "arguments": json.dumps(func_args, ensure_ascii=False)
                    }
                })
                has_tool_calls = True
            elif "functionResponse" in part:
                # å‡½æ•°å“åº”åœ¨è¿™é‡Œæ ‡è®°ï¼Œä½†å®é™…å¤„ç†éœ€è¦åœ¨æ¶ˆæ¯çº§åˆ«
                converted_content.append({
                    "type": "function_response",
                    "function_response": part["functionResponse"]
                })
        
        # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œè¿”å›ç‰¹æ®Šæ ¼å¼æ ‡è¯†
        if has_tool_calls:
            return {
                "type": "tool_calls",
                "content": text_content if text_content else None,
                "tool_calls": tool_calls
            }
        
        # å¦‚æœåªæœ‰ä¸€ä¸ªæ–‡æœ¬ä¸”æ²¡æœ‰å…¶ä»–å†…å®¹ï¼Œç›´æ¥è¿”å›æ–‡æœ¬
        if len(converted_content) == 1 and converted_content[0].get("type") == "text":
            return converted_content[0]["text"]
        
        return converted_content if converted_content else ""
    
    def _convert_content_to_anthropic(self, parts: List[Dict[str, Any]]) -> Any:
        """è½¬æ¢Geminiå†…å®¹åˆ°Anthropicæ ¼å¼"""
        # å¤„ç†å¤šæ¨¡æ€å’Œå·¥å…·è°ƒç”¨å†…å®¹
        anthropic_content = []
        
        for part in parts:
            if "text" in part:
                text_content = part["text"]
                if text_content.strip():  # åªæ·»åŠ éç©ºæ–‡æœ¬
                    # æ£€æŸ¥æ˜¯å¦æ˜¯thinkingå†…å®¹
                    if part.get("thought", False):
                        # Gemini thinking â†’ Anthropic thinking
                        anthropic_content.append({
                            "type": "thinking",
                            "thinking": text_content
                        })
                    else:
                        # æ™®é€šæ–‡æœ¬å†…å®¹
                        anthropic_content.append({
                            "type": "text",
                            "text": text_content
                        })
            elif "inlineData" in part:
                # è½¬æ¢å›¾åƒæ ¼å¼
                inline_data = part["inlineData"]
                anthropic_content.append({
                    "type": "image",
                    "source": {
                        "type": "base64",
                        "media_type": inline_data.get("mimeType", "image/jpeg"),
                        "data": inline_data.get("data", "")
                    }
                })
            elif "functionCall" in part:
                # è½¬æ¢å·¥å…·è°ƒç”¨ (functionCall â†’ tool_use)
                func_call = part["functionCall"]
                func_name = func_call.get("name", "")
                
                # ä½¿ç”¨æ˜ å°„è¡¨è·å–ä¸€è‡´çš„ID
                tool_id = None
                if hasattr(self, '_function_call_mapping') and self._function_call_mapping:
                    # æŸ¥æ‰¾å¯¹åº”çš„ID
                    for key, mapped_id in self._function_call_mapping.items():
                        if key.startswith(func_name) and not key.startswith("response_"):
                            tool_id = mapped_id
                            break
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ å°„IDï¼Œç”Ÿæˆä¸€ä¸ª
                if not tool_id:
                    tool_id = f"call_{func_name}_{hash(str(func_call.get('args', {}))) % 1000000000}"
                
                anthropic_content.append({
                    "type": "tool_use",
                    "id": tool_id,
                    "name": func_name,
                    "input": func_call.get("args", {})
                })
            elif "functionResponse" in part:
                # è½¬æ¢å·¥å…·å“åº” (functionResponse â†’ tool_result)
                func_response = part["functionResponse"]
                func_name = func_response.get("name", "")
                
                # ä½¿ç”¨æ˜ å°„è¡¨è·å–å¯¹åº”çš„tool_use ID
                tool_id = None
                if hasattr(self, '_function_call_mapping') and self._function_call_mapping:
                    for key, mapped_id in self._function_call_mapping.items():
                        if key.startswith(f"response_{func_name}"):
                            tool_id = mapped_id
                            break
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ å°„IDï¼Œå°è¯•ä»functionResponseä¸­è·å–æˆ–ç”Ÿæˆ
                if not tool_id:
                    # æ£€æŸ¥functionResponseæ˜¯å¦åŒ…å«åŸå§‹çš„tool_use_id
                    response_data = func_response.get("response", {})
                    if isinstance(response_data, dict) and "_tool_use_id" in response_data:
                        tool_id = response_data["_tool_use_id"]
                    else:
                        # ç”Ÿæˆä¸€ä¸ªåŸºäºå‡½æ•°åçš„ä¸€è‡´æ€§ID
                        # ä½¿ç”¨å‡½æ•°åä½œä¸ºç§å­æ¥ç”Ÿæˆä¸€è‡´çš„hash
                        import hashlib
                        seed = f"{func_name}_seed"
                        hash_value = int(hashlib.md5(seed.encode()).hexdigest()[:8], 16) % 1000000000
                        tool_id = f"call_{func_name}_{hash_value}"
                
                # æå–å®é™…çš„å·¥å…·ç»“æœå†…å®¹
                result_content = func_response.get("response", {})
                if isinstance(result_content, dict):
                    # å¦‚æœåŒ…å«_tool_use_idï¼Œç§»é™¤å®ƒ
                    result_content = {k: v for k, v in result_content.items() if k != "_tool_use_id"}
                    # æå–å®é™…å†…å®¹
                    actual_content = result_content.get("content", result_content)
                else:
                    actual_content = result_content
                    
                anthropic_content.append({
                    "type": "tool_result",
                    "tool_use_id": tool_id,
                    "content": str(actual_content)
                })
        
        # å¦‚æœæ²¡æœ‰ä»»ä½•æœ‰æ•ˆå†…å®¹ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²ï¼ˆä¼šåœ¨è°ƒç”¨å¤„å¤„ç†ï¼‰
        if not anthropic_content:
            return ""
        
        # å¦‚æœåªæœ‰ä¸€ä¸ªæ–‡æœ¬å†…å®¹ï¼Œç›´æ¥è¿”å›å­—ç¬¦ä¸²
        if len(anthropic_content) == 1 and anthropic_content[0].get("type") == "text":
            return anthropic_content[0]["text"]
        
        # è¿”å›å†…å®¹å—æ•°ç»„
        return anthropic_content
    
    def _sanitize_schema_for_openai(self, schema: Dict[str, Any]) -> Dict[str, Any]:
        """å°†Geminiæ ¼å¼çš„JSON Schemaè½¬æ¢ä¸ºOpenAIå…¼å®¹çš„æ ¼å¼"""
        if not isinstance(schema, dict):
            return schema
        
        # å¤åˆ¶schemaé¿å…ä¿®æ”¹åŸå§‹æ•°æ®
        sanitized = copy.deepcopy(schema)
        
        # Geminiåˆ°OpenAIçš„ç±»å‹æ˜ å°„
        type_mapping = {
            "STRING": "string",
            "NUMBER": "number", 
            "INTEGER": "integer",
            "BOOLEAN": "boolean",
            "ARRAY": "array",
            "OBJECT": "object"
        }
        
        def convert_types(obj):
            """é€’å½’è½¬æ¢schemaä¸­çš„ç±»å‹"""
            if isinstance(obj, dict):
                # è½¬æ¢typeå­—æ®µ
                if "type" in obj and isinstance(obj["type"], str):
                    obj["type"] = type_mapping.get(obj["type"].upper(), obj["type"].lower())
                
                # è½¬æ¢éœ€è¦æ•´æ•°å€¼çš„å­—æ®µï¼ˆå°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•´æ•°ï¼‰
                integer_fields = ["minItems", "maxItems", "minimum", "maximum", "minLength", "maxLength"]
                for field in integer_fields:
                    if field in obj and isinstance(obj[field], str) and obj[field].isdigit():
                        obj[field] = int(obj[field])
                
                # é€’å½’å¤„ç†æ‰€æœ‰å­—æ®µï¼ˆè·³è¿‡å·²ç»å¤„ç†è¿‡çš„æ ‡é‡å­—æ®µï¼‰
                for key, value in obj.items():
                    if key not in ["type"] + integer_fields:  # é¿å…é‡å¤å¤„ç†å·²è½¬æ¢çš„å­—æ®µ
                        obj[key] = convert_types(value)
                    
            elif isinstance(obj, list):
                # å¤„ç†æ•°ç»„ä¸­çš„æ¯ä¸ªå…ƒç´ 
                return [convert_types(item) for item in obj]
            
            return obj
        
        return convert_types(sanitized)
    
    def _build_function_call_mapping(self, contents: List[Dict[str, Any]]) -> Dict[str, str]:
        """æ‰«ææ•´ä¸ªå¯¹è¯å†å²ï¼Œä¸ºfunctionCallå’ŒfunctionResponseå»ºç«‹IDæ˜ å°„"""
        mapping = {}
        function_call_sequence = {}  # {func_name: sequence_number}
        
        for content in contents:
            parts = content.get("parts", [])
            for part in parts:
                if "functionCall" in part:
                    func_name = part["functionCall"].get("name", "")
                    if func_name:
                        # ä¸ºæ¯ä¸ªå‡½æ•°è°ƒç”¨ç”Ÿæˆå”¯ä¸€çš„sequence number
                        sequence = function_call_sequence.get(func_name, 0) + 1
                        function_call_sequence[func_name] = sequence
                        
                        # ç”Ÿæˆä¸€è‡´çš„ID
                        tool_call_id = f"call_{func_name}_{sequence:04d}"
                        mapping[f"{func_name}_{sequence}"] = tool_call_id
                        
                elif "functionResponse" in part:
                    func_name = part["functionResponse"].get("name", "")
                    if func_name:
                        # ä¸ºfunctionResponseåˆ†é…æœ€è¿‘çš„functionCallçš„ID
                        current_sequence = function_call_sequence.get(func_name, 0)
                        if current_sequence > 0:
                            mapping[f"response_{func_name}_{current_sequence}"] = mapping.get(f"{func_name}_{current_sequence}")
        
        return mapping
    
    def _convert_schema_for_anthropic(self, schema: Dict[str, Any]) -> Dict[str, Any]:
        """å°†Geminiæ ¼å¼çš„JSON Schemaè½¬æ¢ä¸ºAnthropicå…¼å®¹çš„æ ¼å¼"""
        if not isinstance(schema, dict):
            return schema
        
        # å¤åˆ¶schemaé¿å…ä¿®æ”¹åŸå§‹æ•°æ®
        converted = copy.deepcopy(schema)
        
        # Geminiåˆ°Anthropicçš„ç±»å‹æ˜ å°„
        type_mapping = {
            "STRING": "string",
            "NUMBER": "number", 
            "INTEGER": "integer",
            "BOOLEAN": "boolean",
            "ARRAY": "array",
            "OBJECT": "object"
        }
        
        def convert_types(obj):
            """é€’å½’è½¬æ¢schemaä¸­çš„ç±»å‹"""
            if isinstance(obj, dict):
                # è½¬æ¢typeå­—æ®µ
                if "type" in obj and isinstance(obj["type"], str):
                    obj["type"] = type_mapping.get(obj["type"].upper(), obj["type"].lower())
                
                # è½¬æ¢éœ€è¦æ•´æ•°å€¼çš„å­—æ®µï¼ˆå°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•´æ•°ï¼‰
                integer_fields = ["minItems", "maxItems", "minimum", "maximum", "minLength", "maxLength"]
                for field in integer_fields:
                    if field in obj and isinstance(obj[field], str) and obj[field].isdigit():
                        obj[field] = int(obj[field])
                
                # é€’å½’å¤„ç†æ‰€æœ‰å­—æ®µï¼ˆè·³è¿‡å·²ç»å¤„ç†è¿‡çš„æ ‡é‡å­—æ®µï¼‰
                for key, value in obj.items():
                    if key not in ["type"] + integer_fields:  # é¿å…é‡å¤å¤„ç†å·²è½¬æ¢çš„å­—æ®µ
                        obj[key] = convert_types(value)
                    
            elif isinstance(obj, list):
                # å¤„ç†æ•°ç»„ä¸­çš„æ¯ä¸ªå…ƒç´ 
                return [convert_types(item) for item in obj]
            
            return obj
        
        return convert_types(converted)

    def _map_finish_reason(self, reason: str, source_format: str, target_format: str) -> str:
        """æ˜ å°„ç»“æŸåŸå› """
        reason_mappings = {
            "openai": {
                "gemini": {
                    "stop": "STOP",
                    "length": "MAX_TOKENS",
                    "content_filter": "SAFETY",
                    "tool_calls": "MODEL_REQUESTED_TOOL"
                }
            },
            "anthropic": {
                "gemini": {
                    "end_turn": "STOP",
                    "max_tokens": "MAX_TOKENS",
                    "stop_sequence": "STOP",
                    "tool_use": "STOP"
                }
            }
        }
        
        try:
            return reason_mappings[source_format][target_format].get(reason, "STOP")
        except KeyError:
            return "STOP"